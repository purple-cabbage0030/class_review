{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"Process.ipynb","provenance":[],"collapsed_sections":["rc3czwIt6fcF","M07MPnxl6fcG","VlXp-kA_6fcO"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3eeFf_vR6fbk"},"source":["# Tensorflow Object Detection API\n","- Tensorflow Object Detection API는 TensorFlow를 이용해서 Object Detection 모델을 train하고 deploy하는 것을 쉽게 도와주는 오픈소스 프레임워크.\n","- https://github.com/tensorflow/models/tree/master/research/object_detection\n","- Tutorial: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/"]},{"cell_type":"markdown","metadata":{"id":"OTW8EmED6fb1"},"source":["# Custom (Image) Data 구하기"]},{"cell_type":"markdown","metadata":{"id":"qfcUp3XQ6fb5"},"source":["# Custom (Image) Data Labeling"]},{"cell_type":"markdown","metadata":{"id":"nfC0wvQi6fb6"},"source":["# 전단계\n","- 구글드라이브 연결\n","- raw_data의 데이터압축파일을 VM local에 압축 푼다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1wOv7rPR6fb7","executionInfo":{"status":"ok","timestamp":1632891661778,"user_tz":-540,"elapsed":17671,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"5396e580-92bf-43fc-a8b4-04280eb47f4c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"9D8RZiKW7JYb","executionInfo":{"status":"ok","timestamp":1632891665485,"user_tz":-540,"elapsed":2504,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["# 압축 풀기\n","# unzip 명령어 이용: !unzip 압축파일 -d 압축풀디렉토리\n","!unzip -q /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/raw_data/american_sign_language_letters.zip -d images"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m5qXwds46fb9"},"source":["# Tensorflow Object Detection 2 API 설치\n","1. clone \n","    - `!git clone https://github.com/tensorflow/models.git`\n","1. PYTHONPATH 환경설정에 models/research 추가  \n","1. 필요 모듈 설치\n","    - `!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk`\n","    - `!pip install -qq Cython contextlib2 pillow lxml matplotlib pycocotools`\n","1. proto 파일 컴파일\n","    - models/research 경로로 이동\n","        - `%cd models/research`\n","    - `!protoc object_detection/protos/*.proto --python_out=.`\n","1. setup.py 를 이용해 필요한 모듈 추가 설치\n","    - setup.py를 현재 디렉토리로 카피\n","        - `!cp object_detection/packages/tf2/setup.py . `\n","    - 설치\n","        - `!python -m pip install . `\n","    - 설치 확인 - 아래 스크립트 실행시 오류 없이 실행되면 설치 잘 된 것임.\n","        - `!python object_detection/builders/model_builder_tf2_test.py`\n","1. 원래 디렉토리로 이동\n","    - `%cd ../..`        "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59C7tUB46fb_","executionInfo":{"status":"ok","timestamp":1632891701602,"user_tz":-540,"elapsed":22610,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"6220d263-fe05-46d6-f6e0-a35919e7fb60"},"source":["# Tensorflow Object Detection API2 를 clone\n","!git clone https://github.com/tensorflow/models.git"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'models'...\n","remote: Enumerating objects: 63918, done.\u001b[K\n","remote: Counting objects: 100% (423/423), done.\u001b[K\n","remote: Compressing objects: 100% (183/183), done.\u001b[K\n","remote: Total 63918 (delta 252), reused 408 (delta 240), pack-reused 63495\u001b[K\n","Receiving objects: 100% (63918/63918), 574.98 MiB | 30.39 MiB/s, done.\n","Resolving deltas: 100% (44670/44670), done.\n"]}]},{"cell_type":"code","metadata":{"id":"erBMO8YY6fcA","executionInfo":{"status":"ok","timestamp":1632891703184,"user_tz":-540,"elapsed":6,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["# 환경설정 - PYTHONPATH = models/research\n","import os\n","os.environ['PYTHONPATH'] += \":/content/models/research\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oTkr-kx56fcB","executionInfo":{"status":"ok","timestamp":1632891717731,"user_tz":-540,"elapsed":5300,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"cfbbafc4-48d3-4707-95f0-8d9872e593a1"},"source":["# 추가 필요 모듈 설치\n","!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Selecting previously unselected package python-bs4.\n","(Reading database ... 155013 files and directories currently installed.)\n","Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n","Unpacking python-bs4 (4.6.0-1) ...\n","Selecting previously unselected package python-pkg-resources.\n","Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n","Unpacking python-pkg-resources (39.0.1-2) ...\n","Selecting previously unselected package python-chardet.\n","Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n","Unpacking python-chardet (3.0.4-1) ...\n","Selecting previously unselected package python-six.\n","Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n","Unpacking python-six (1.11.0-2) ...\n","Selecting previously unselected package python-webencodings.\n","Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n","Unpacking python-webencodings (0.5-2) ...\n","Selecting previously unselected package python-html5lib.\n","Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n","Unpacking python-html5lib (0.999999999-1) ...\n","Selecting previously unselected package python-lxml:amd64.\n","Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.4_amd64.deb ...\n","Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.4) ...\n","Selecting previously unselected package python-olefile.\n","Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n","Unpacking python-olefile (0.45.1-1) ...\n","Selecting previously unselected package python-pil:amd64.\n","Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.6_amd64.deb ...\n","Unpacking python-pil:amd64 (5.1.0-1ubuntu0.6) ...\n","Setting up python-pkg-resources (39.0.1-2) ...\n","Setting up python-six (1.11.0-2) ...\n","Setting up python-bs4 (4.6.0-1) ...\n","Setting up python-lxml:amd64 (4.2.1-1ubuntu0.4) ...\n","Setting up python-olefile (0.45.1-1) ...\n","Setting up python-pil:amd64 (5.1.0-1ubuntu0.6) ...\n","Setting up python-webencodings (0.5-2) ...\n","Setting up python-chardet (3.0.4-1) ...\n","Setting up python-html5lib (0.999999999-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}]},{"cell_type":"code","metadata":{"id":"0KoebZ0z6fcB","executionInfo":{"status":"ok","timestamp":1632891726444,"user_tz":-540,"elapsed":3986,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["!pip install -qq Cython contextlib2 pillow lxml matplotlib pycocotools"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJEyJHsE6fcC","executionInfo":{"status":"ok","timestamp":1632891730630,"user_tz":-540,"elapsed":268,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"e33eb8e5-eba1-4374-d61f-60eb5751d1ef"},"source":["# proto 파일 컴파일\n","%cd /content/models/research"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/models/research\n"]}]},{"cell_type":"code","metadata":{"id":"BLNq07-a6fcC","executionInfo":{"status":"ok","timestamp":1632891732679,"user_tz":-540,"elapsed":272,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["!protoc object_detection/protos/*.proto --python_out=."],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"951XungB6fcD","executionInfo":{"status":"ok","timestamp":1632891737589,"user_tz":-540,"elapsed":255,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["# setup.py 실행\n","# setup.py를 현재 디렉토리로 카피 - !cp 대상파일경로 카피경로\n","!cp object_detection/packages/tf2/setup.py ."],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kqafSAqp7pOI","executionInfo":{"status":"ok","timestamp":1632891782965,"user_tz":-540,"elapsed":36334,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"7eed9ac6-f12f-405b-8c1b-544ce8a7f355"},"source":["# setup.py를 이용해 추가 패키지 설치\n","!python -m pip install ."],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing /content/models/research\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Collecting avro-python3\n","  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n","Collecting apache-beam\n","  Downloading apache_beam-2.32.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n","\u001b[K     |████████████████████████████████| 9.8 MB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.24)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n","Collecting tf-slim\n","  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","\u001b[K     |████████████████████████████████| 352 kB 51.2 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.2)\n","Collecting lvis\n","  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.5)\n","Collecting tf-models-official>=2.5.1\n","  Downloading tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 40.4 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 35.4 MB/s \n","\u001b[?25hCollecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 414 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.19.5)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n","Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n","\u001b[K     |████████████████████████████████| 90 kB 10.2 MB/s \n","\u001b[?25hCollecting tensorflow-text>=2.5.0\n","  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 30.6 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 41.0 MB/s \n","\u001b[?25hCollecting py-cpuinfo>=3.3.0\n","  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n","\u001b[K     |████████████████████████████████| 99 kB 9.6 MB/s \n","\u001b[?25hCollecting tensorflow-addons\n","  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 32.6 MB/s \n","\u001b[?25hCollecting opencv-python-headless\n","  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n","\u001b[K     |████████████████████████████████| 37.1 MB 58 kB/s \n","\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n","Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n","Collecting tensorflow-model-optimization>=0.4.1\n","  Downloading tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211 kB)\n","\u001b[K     |████████████████████████████████| 211 kB 47.0 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n","Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.8)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n","Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n","Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.26.3)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n","Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.53.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2018.9)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.7.2)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (5.0.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.62.3)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.5.30)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.4.7)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.0)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12.1)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n","Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.7.4.3)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n","Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (5.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n","Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.40.0)\n","Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (4.8.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.1)\n","Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.6)\n","Requirement already satisfied: pyarrow<5.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.0.0)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n","Collecting orjson<4.0\n","  Downloading orjson-3.6.3-cp37-cp37m-manylinux_2_24_x86_64.whl (234 kB)\n","\u001b[K     |████████████████████████████████| 234 kB 44.4 MB/s \n","\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","\u001b[K     |████████████████████████████████| 151 kB 47.3 MB/s \n","\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n","  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n","Collecting avro-python3\n","  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n","Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.12.0)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n","Collecting future<1.0.0,>=0.18.2\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 42.0 MB/s \n","\u001b[?25hCollecting requests<3.0.0dev,>=2.18.0\n","  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 713 kB/s \n","\u001b[?25hCollecting fastavro<2,>=0.21.4\n","  Downloading fastavro-1.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 41.5 MB/s \n","\u001b[?25hRequirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.5.0)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.3.2)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.10.0)\n","Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n","Collecting portalocker\n","  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2019.12.20)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.2.2)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.2.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n","Building wheels for collected packages: object-detection, py-cpuinfo, avro-python3, dill, future, seqeval\n","  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1665976 sha256=c1ddc4239ab2281aa6662571a21a86864e7ff2fea75ee387d1c4d866ed915aab\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-sd7tu4bw/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n","  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=2d3d83033ad61a6d360b625026e654d4060944c4db66751889f643a3e72d0000\n","  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n","  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=2e33f3f5e3fb5b24bc568b685f587ef86c7fe8f38d4133769b17010189d5d428\n","  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n","  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=611e302da705b0ec29a0d2ded1722c088645080a391f7857b067bc6b631ab98e\n","  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=349be54dcb3e3cff45300f6bc4fdc27b8462dbd19feb45ca55588309c5a39e0d\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=b2d5bff47cd0eb14ca0c14e529d4d4124975c2aaede34062a6b45e896309b7de\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built object-detection py-cpuinfo avro-python3 dill future seqeval\n","Installing collected packages: requests, portalocker, future, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, py-cpuinfo, orjson, opencv-python-headless, hdfs, fastavro, avro-python3, tf-models-official, lvis, apache-beam, object-detection\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.4\n","    Uninstalling dill-0.3.4:\n","      Successfully uninstalled dill-0.3.4\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed apache-beam-2.32.0 avro-python3-1.9.2.1 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.5 future-0.18.2 hdfs-2.6.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.5.3.56 orjson-3.6.3 portalocker-2.3.2 py-cpuinfo-8.0.0 pyyaml-5.4.1 requests-2.26.0 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-addons-0.14.0 tensorflow-model-optimization-0.6.0 tensorflow-text-2.6.0 tf-models-official-2.6.0 tf-slim-1.1.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P9zGbyEu7rfL","executionInfo":{"status":"ok","timestamp":1632891828937,"user_tz":-540,"elapsed":43047,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"7f4b6789-a548-4d9e-aa16-23630154bc2b"},"source":["# 설치 확인\n","!python object_detection/builders/model_builder_tf2_test.py"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-09-29 05:03:07.408987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:03:07.875144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:03:07.875992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","Running tests under Python 3.7.12: /usr/bin/python3\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n","2021-09-29 05:03:07.891797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:03:07.892609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:03:07.893360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:03:13.168381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:03:13.169228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:03:13.169952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:03:13.170706: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2021-09-29 05:03:13.170799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10819 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n","W0929 05:03:13.543184 139943061514112 model_builder.py:1091] Building experimental DeepMAC meta-arch. Some features may be omitted.\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 6.09s\n","I0929 05:03:13.974763 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 6.09s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.61s\n","I0929 05:03:14.590199 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.61s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.34s\n","I0929 05:03:14.929073 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.34s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.33s\n","I0929 05:03:15.258529 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.33s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.22s\n","I0929 05:03:17.476804 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.22s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n","[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n","I0929 05:03:17.478192 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.03s\n","I0929 05:03:17.504473 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.03s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n","I0929 05:03:17.522712 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n","I0929 05:03:17.541194 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.12s\n","I0929 05:03:17.660829 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.12s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.12s\n","I0929 05:03:17.778149 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.12s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.12s\n","I0929 05:03:17.896038 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.12s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.12s\n","I0929 05:03:18.019222 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.12s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.11s\n","I0929 05:03:18.132453 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.11s\n","[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n","[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n","I0929 05:03:18.165292 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n","[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n","[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n","I0929 05:03:18.537096 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b0\n","I0929 05:03:18.537296 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n","I0929 05:03:18.537418 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 3\n","I0929 05:03:18.539898 139943061514112 efficientnet_model.py:147] round_filter input=32 output=32\n","I0929 05:03:18.560754 139943061514112 efficientnet_model.py:147] round_filter input=32 output=32\n","I0929 05:03:18.560927 139943061514112 efficientnet_model.py:147] round_filter input=16 output=16\n","I0929 05:03:18.640501 139943061514112 efficientnet_model.py:147] round_filter input=16 output=16\n","I0929 05:03:18.640741 139943061514112 efficientnet_model.py:147] round_filter input=24 output=24\n","I0929 05:03:18.825480 139943061514112 efficientnet_model.py:147] round_filter input=24 output=24\n","I0929 05:03:18.825671 139943061514112 efficientnet_model.py:147] round_filter input=40 output=40\n","I0929 05:03:19.002861 139943061514112 efficientnet_model.py:147] round_filter input=40 output=40\n","I0929 05:03:19.003052 139943061514112 efficientnet_model.py:147] round_filter input=80 output=80\n","I0929 05:03:19.278215 139943061514112 efficientnet_model.py:147] round_filter input=80 output=80\n","I0929 05:03:19.278405 139943061514112 efficientnet_model.py:147] round_filter input=112 output=112\n","I0929 05:03:19.563018 139943061514112 efficientnet_model.py:147] round_filter input=112 output=112\n","I0929 05:03:19.563248 139943061514112 efficientnet_model.py:147] round_filter input=192 output=192\n","I0929 05:03:19.934123 139943061514112 efficientnet_model.py:147] round_filter input=192 output=192\n","I0929 05:03:19.934339 139943061514112 efficientnet_model.py:147] round_filter input=320 output=320\n","I0929 05:03:20.021842 139943061514112 efficientnet_model.py:147] round_filter input=1280 output=1280\n","I0929 05:03:20.060069 139943061514112 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I0929 05:03:20.118638 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b1\n","I0929 05:03:20.118842 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 88\n","I0929 05:03:20.118956 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 4\n","I0929 05:03:20.120950 139943061514112 efficientnet_model.py:147] round_filter input=32 output=32\n","I0929 05:03:20.139057 139943061514112 efficientnet_model.py:147] round_filter input=32 output=32\n","I0929 05:03:20.139278 139943061514112 efficientnet_model.py:147] round_filter input=16 output=16\n","I0929 05:03:20.295976 139943061514112 efficientnet_model.py:147] round_filter input=16 output=16\n","I0929 05:03:20.296220 139943061514112 efficientnet_model.py:147] round_filter input=24 output=24\n","I0929 05:03:20.571744 139943061514112 efficientnet_model.py:147] round_filter input=24 output=24\n","I0929 05:03:20.571956 139943061514112 efficientnet_model.py:147] round_filter input=40 output=40\n","I0929 05:03:20.845383 139943061514112 efficientnet_model.py:147] round_filter input=40 output=40\n","I0929 05:03:20.845563 139943061514112 efficientnet_model.py:147] round_filter input=80 output=80\n","I0929 05:03:21.202382 139943061514112 efficientnet_model.py:147] round_filter input=80 output=80\n","I0929 05:03:21.202592 139943061514112 efficientnet_model.py:147] round_filter input=112 output=112\n","I0929 05:03:21.583421 139943061514112 efficientnet_model.py:147] round_filter input=112 output=112\n","I0929 05:03:21.583630 139943061514112 efficientnet_model.py:147] round_filter input=192 output=192\n","I0929 05:03:22.044890 139943061514112 efficientnet_model.py:147] round_filter input=192 output=192\n","I0929 05:03:22.045084 139943061514112 efficientnet_model.py:147] round_filter input=320 output=320\n","I0929 05:03:22.229214 139943061514112 efficientnet_model.py:147] round_filter input=1280 output=1280\n","I0929 05:03:22.270994 139943061514112 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I0929 05:03:22.481330 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b2\n","I0929 05:03:22.481548 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 112\n","I0929 05:03:22.481667 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 5\n","I0929 05:03:22.483535 139943061514112 efficientnet_model.py:147] round_filter input=32 output=32\n","I0929 05:03:22.501102 139943061514112 efficientnet_model.py:147] round_filter input=32 output=32\n","I0929 05:03:22.501240 139943061514112 efficientnet_model.py:147] round_filter input=16 output=16\n","I0929 05:03:22.647967 139943061514112 efficientnet_model.py:147] round_filter input=16 output=16\n","I0929 05:03:22.648162 139943061514112 efficientnet_model.py:147] round_filter input=24 output=24\n","I0929 05:03:22.917229 139943061514112 efficientnet_model.py:147] round_filter input=24 output=24\n","I0929 05:03:22.917454 139943061514112 efficientnet_model.py:147] round_filter input=40 output=48\n","I0929 05:03:23.184927 139943061514112 efficientnet_model.py:147] round_filter input=40 output=48\n","I0929 05:03:23.185182 139943061514112 efficientnet_model.py:147] round_filter input=80 output=88\n","I0929 05:03:23.566302 139943061514112 efficientnet_model.py:147] round_filter input=80 output=88\n","I0929 05:03:23.566527 139943061514112 efficientnet_model.py:147] round_filter input=112 output=120\n","I0929 05:03:23.948093 139943061514112 efficientnet_model.py:147] round_filter input=112 output=120\n","I0929 05:03:23.948405 139943061514112 efficientnet_model.py:147] round_filter input=192 output=208\n","I0929 05:03:24.436351 139943061514112 efficientnet_model.py:147] round_filter input=192 output=208\n","I0929 05:03:24.436585 139943061514112 efficientnet_model.py:147] round_filter input=320 output=352\n","I0929 05:03:24.619981 139943061514112 efficientnet_model.py:147] round_filter input=1280 output=1408\n","I0929 05:03:24.655631 139943061514112 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I0929 05:03:24.724101 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b3\n","I0929 05:03:24.724302 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 160\n","I0929 05:03:24.724406 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 6\n","I0929 05:03:24.726330 139943061514112 efficientnet_model.py:147] round_filter input=32 output=40\n","I0929 05:03:24.753046 139943061514112 efficientnet_model.py:147] round_filter input=32 output=40\n","I0929 05:03:24.753269 139943061514112 efficientnet_model.py:147] round_filter input=16 output=24\n","I0929 05:03:24.900245 139943061514112 efficientnet_model.py:147] round_filter input=16 output=24\n","I0929 05:03:24.900450 139943061514112 efficientnet_model.py:147] round_filter input=24 output=32\n","I0929 05:03:25.174348 139943061514112 efficientnet_model.py:147] round_filter input=24 output=32\n","I0929 05:03:25.174585 139943061514112 efficientnet_model.py:147] round_filter input=40 output=48\n","I0929 05:03:25.457484 139943061514112 efficientnet_model.py:147] round_filter input=40 output=48\n","I0929 05:03:25.457714 139943061514112 efficientnet_model.py:147] round_filter input=80 output=96\n","I0929 05:03:25.907537 139943061514112 efficientnet_model.py:147] round_filter input=80 output=96\n","I0929 05:03:25.907736 139943061514112 efficientnet_model.py:147] round_filter input=112 output=136\n","I0929 05:03:26.374450 139943061514112 efficientnet_model.py:147] round_filter input=112 output=136\n","I0929 05:03:26.374642 139943061514112 efficientnet_model.py:147] round_filter input=192 output=232\n","I0929 05:03:26.939217 139943061514112 efficientnet_model.py:147] round_filter input=192 output=232\n","I0929 05:03:26.939429 139943061514112 efficientnet_model.py:147] round_filter input=320 output=384\n","I0929 05:03:27.117134 139943061514112 efficientnet_model.py:147] round_filter input=1280 output=1536\n","I0929 05:03:27.153074 139943061514112 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I0929 05:03:27.480805 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b4\n","I0929 05:03:27.481014 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 224\n","I0929 05:03:27.481127 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 7\n","I0929 05:03:27.483127 139943061514112 efficientnet_model.py:147] round_filter input=32 output=48\n","I0929 05:03:27.501653 139943061514112 efficientnet_model.py:147] round_filter input=32 output=48\n","I0929 05:03:27.501812 139943061514112 efficientnet_model.py:147] round_filter input=16 output=24\n","I0929 05:03:27.648763 139943061514112 efficientnet_model.py:147] round_filter input=16 output=24\n","I0929 05:03:27.649039 139943061514112 efficientnet_model.py:147] round_filter input=24 output=32\n","I0929 05:03:28.019474 139943061514112 efficientnet_model.py:147] round_filter input=24 output=32\n","I0929 05:03:28.019673 139943061514112 efficientnet_model.py:147] round_filter input=40 output=56\n","I0929 05:03:28.389506 139943061514112 efficientnet_model.py:147] round_filter input=40 output=56\n","I0929 05:03:28.389727 139943061514112 efficientnet_model.py:147] round_filter input=80 output=112\n","I0929 05:03:28.934162 139943061514112 efficientnet_model.py:147] round_filter input=80 output=112\n","I0929 05:03:28.934448 139943061514112 efficientnet_model.py:147] round_filter input=112 output=160\n","I0929 05:03:29.504902 139943061514112 efficientnet_model.py:147] round_filter input=112 output=160\n","I0929 05:03:29.505109 139943061514112 efficientnet_model.py:147] round_filter input=192 output=272\n","I0929 05:03:30.210063 139943061514112 efficientnet_model.py:147] round_filter input=192 output=272\n","I0929 05:03:30.210322 139943061514112 efficientnet_model.py:147] round_filter input=320 output=448\n","I0929 05:03:30.396668 139943061514112 efficientnet_model.py:147] round_filter input=1280 output=1792\n","I0929 05:03:30.429164 139943061514112 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I0929 05:03:30.523419 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b5\n","I0929 05:03:30.523641 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 288\n","I0929 05:03:30.523761 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 7\n","I0929 05:03:30.525923 139943061514112 efficientnet_model.py:147] round_filter input=32 output=48\n","I0929 05:03:30.545272 139943061514112 efficientnet_model.py:147] round_filter input=32 output=48\n","I0929 05:03:30.545437 139943061514112 efficientnet_model.py:147] round_filter input=16 output=24\n","I0929 05:03:30.776414 139943061514112 efficientnet_model.py:147] round_filter input=16 output=24\n","I0929 05:03:30.776677 139943061514112 efficientnet_model.py:147] round_filter input=24 output=40\n","I0929 05:03:31.225486 139943061514112 efficientnet_model.py:147] round_filter input=24 output=40\n","I0929 05:03:31.225690 139943061514112 efficientnet_model.py:147] round_filter input=40 output=64\n","I0929 05:03:31.694835 139943061514112 efficientnet_model.py:147] round_filter input=40 output=64\n","I0929 05:03:31.695065 139943061514112 efficientnet_model.py:147] round_filter input=80 output=128\n","I0929 05:03:32.323668 139943061514112 efficientnet_model.py:147] round_filter input=80 output=128\n","I0929 05:03:32.323936 139943061514112 efficientnet_model.py:147] round_filter input=112 output=176\n","I0929 05:03:33.237017 139943061514112 efficientnet_model.py:147] round_filter input=112 output=176\n","I0929 05:03:33.237234 139943061514112 efficientnet_model.py:147] round_filter input=192 output=304\n","I0929 05:03:34.111894 139943061514112 efficientnet_model.py:147] round_filter input=192 output=304\n","I0929 05:03:34.112106 139943061514112 efficientnet_model.py:147] round_filter input=320 output=512\n","I0929 05:03:34.390690 139943061514112 efficientnet_model.py:147] round_filter input=1280 output=2048\n","I0929 05:03:34.429779 139943061514112 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I0929 05:03:34.541351 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b6\n","I0929 05:03:34.541592 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n","I0929 05:03:34.541704 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 8\n","I0929 05:03:34.543762 139943061514112 efficientnet_model.py:147] round_filter input=32 output=56\n","I0929 05:03:34.562571 139943061514112 efficientnet_model.py:147] round_filter input=32 output=56\n","I0929 05:03:34.562741 139943061514112 efficientnet_model.py:147] round_filter input=16 output=32\n","I0929 05:03:34.781320 139943061514112 efficientnet_model.py:147] round_filter input=16 output=32\n","I0929 05:03:34.781516 139943061514112 efficientnet_model.py:147] round_filter input=24 output=40\n","I0929 05:03:35.315604 139943061514112 efficientnet_model.py:147] round_filter input=24 output=40\n","I0929 05:03:35.315844 139943061514112 efficientnet_model.py:147] round_filter input=40 output=72\n","I0929 05:03:35.873467 139943061514112 efficientnet_model.py:147] round_filter input=40 output=72\n","I0929 05:03:35.873663 139943061514112 efficientnet_model.py:147] round_filter input=80 output=144\n","I0929 05:03:36.626513 139943061514112 efficientnet_model.py:147] round_filter input=80 output=144\n","I0929 05:03:36.626732 139943061514112 efficientnet_model.py:147] round_filter input=112 output=200\n","I0929 05:03:37.370359 139943061514112 efficientnet_model.py:147] round_filter input=112 output=200\n","I0929 05:03:37.370588 139943061514112 efficientnet_model.py:147] round_filter input=192 output=344\n","I0929 05:03:38.662374 139943061514112 efficientnet_model.py:147] round_filter input=192 output=344\n","I0929 05:03:38.662603 139943061514112 efficientnet_model.py:147] round_filter input=320 output=576\n","I0929 05:03:38.927388 139943061514112 efficientnet_model.py:147] round_filter input=1280 output=2304\n","I0929 05:03:38.960699 139943061514112 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I0929 05:03:39.082717 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b7\n","I0929 05:03:39.082921 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n","I0929 05:03:39.083026 139943061514112 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 8\n","I0929 05:03:39.084956 139943061514112 efficientnet_model.py:147] round_filter input=32 output=64\n","I0929 05:03:39.102190 139943061514112 efficientnet_model.py:147] round_filter input=32 output=64\n","I0929 05:03:39.102343 139943061514112 efficientnet_model.py:147] round_filter input=16 output=32\n","I0929 05:03:39.394520 139943061514112 efficientnet_model.py:147] round_filter input=16 output=32\n","I0929 05:03:39.394743 139943061514112 efficientnet_model.py:147] round_filter input=24 output=48\n","I0929 05:03:40.039035 139943061514112 efficientnet_model.py:147] round_filter input=24 output=48\n","I0929 05:03:40.039325 139943061514112 efficientnet_model.py:147] round_filter input=40 output=80\n","I0929 05:03:40.700886 139943061514112 efficientnet_model.py:147] round_filter input=40 output=80\n","I0929 05:03:40.701086 139943061514112 efficientnet_model.py:147] round_filter input=80 output=160\n","I0929 05:03:41.691000 139943061514112 efficientnet_model.py:147] round_filter input=80 output=160\n","I0929 05:03:41.691258 139943061514112 efficientnet_model.py:147] round_filter input=112 output=224\n","I0929 05:03:42.621525 139943061514112 efficientnet_model.py:147] round_filter input=112 output=224\n","I0929 05:03:42.621731 139943061514112 efficientnet_model.py:147] round_filter input=192 output=384\n","I0929 05:03:43.862334 139943061514112 efficientnet_model.py:147] round_filter input=192 output=384\n","I0929 05:03:43.862559 139943061514112 efficientnet_model.py:147] round_filter input=320 output=640\n","I0929 05:03:44.524549 139943061514112 efficientnet_model.py:147] round_filter input=1280 output=2560\n","I0929 05:03:44.559406 139943061514112 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 26.55s\n","I0929 05:03:44.715535 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 26.55s\n","[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n","I0929 05:03:44.722889 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n","I0929 05:03:44.724781 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n","I0929 05:03:44.725397 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n","I0929 05:03:44.727174 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n","[ RUN      ] ModelBuilderTF2Test.test_session\n","[  SKIPPED ] ModelBuilderTF2Test.test_session\n","[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n","I0929 05:03:44.728823 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n","[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n","I0929 05:03:44.729358 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n","[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n","I0929 05:03:44.730611 139943061514112 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n","----------------------------------------------------------------------\n","Ran 24 tests in 36.851s\n","\n","OK (skipped=1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"at0Wxz1L7qnu","executionInfo":{"status":"ok","timestamp":1632891830219,"user_tz":-540,"elapsed":414,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"cfce546e-7922-44ac-d719-dd62e466df74"},"source":["# 작업 디렉토리 /content로 이동\n","%cd ../..\n","!pwd"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","/content\n"]}]},{"cell_type":"markdown","metadata":{"id":"tFlaTvSq6fcD"},"source":["# 경로 설정"]},{"cell_type":"code","metadata":{"id":"AvpOyoda6fcE","executionInfo":{"status":"ok","timestamp":1632891834473,"user_tz":-540,"elapsed":243,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["# root 경로 (workspace)\n","BASE_PATH = \"/content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace\"\n","# utility 기능을 구현한 python script 파일들이 있는 디렉토리\n","SCRIPTS_PATH = \"/content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/scripts\"\n","# Tensorflow Object Detection API2 경로\n","TF_OD_API_PATH = \"/content/models\"\n","# image/annotation 파일들 경로\n","IMAGE_PATH = \"/content/images\"\n","\n","# LabelMap 파일을 저장할 디렉토리 / 파일 경로\n","# label map 설정파일 = 분류할 class들을 정의한 파일\n","LABEL_MAP_PATH = os.path.join(BASE_PATH, 'labelmap')\n","LABEL_MAP_FILE_PATH = os.path.join(LABEL_MAP_PATH, 'label_map.pbtxt')\n","\n","# TF Record 저장 경로\n","# dataset 관련 파일들은 google drive보다 local에 저장\n","TF_RECORD_PATH = \"/content/tfrecord\"\n","if not os.path.isdir(TF_RECORD_PATH):\n","    os.mkdir(TF_RECORD_PATH)\n","\n","# custom dataset을 학습한 모델(모델, weight 파일)을 저장할 경로\n","MODEL_PATH = os.path.join(BASE_PATH, 'model')\n","CHECK_POINT_PATH = os.path.join(MODEL_PATH, 'checkpoint')   # weights(파라미터) 저장할 디렉토리\n","EXPORT_MODEL_PATH = os.path.join(MODEL_PATH, 'export_model')   # 학습된 모델(모델+weight)을 추출해 저장할 디렉토리\n","# pipeline.config 파일 = 모델구조, 학습에 필요한 정보, 평가 시 필요한 정보를 설정하는 파일.\n","PIPELINE_CONFIG_PATH = os.path.join(MODEL_PATH, 'pipeline.config')\n","\n","# 전이학습에 사용할 다운받은 모델을 저장할 디렉토리\n","PRE_TRAINED_MODEL_PATH = os.path.join(BASE_PATH, 'pre_trained_model')"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rc3czwIt6fcF"},"source":["# Custom data 학습 시키기\n","\n","## 다음 세가지 작업이 필요\n","<span style='font-weight:bold;font-size:1.3em'>1. Label Map 파일 생성</span>\n","- 분류 하고자 하는 object의 class와 그 class id 를 pbtxt text 파일로 작성\n","- `models\\research\\object_detection\\data`\n","\n","```\n","item {\n","  id: 1\n","  name: 'aeroplane'\n","}\n","\n","item {\n","  id: 2\n","  name: 'bicycle'\n","}\n","...\n","```\n","\n","<span style='font-weight:bold;font-size:1.3em'>2. pipeline.config</span>\n","- Model을 학습, 검증하기 위해 필요한 설정을 하는 파일\n","- `models\\research\\object_detection\\samples\\configs`\n","\n","<span style='font-weight:bold;font-size:1.3em'>3. 학습/검증/테스트에 사용할 데이터셋을 TFRecord 로 구성</span>\n","- 주요 데이터셋을 TFRecord로 생성하는 코드\n","- `models\\research\\object_detection\\dataset_tools`"]},{"cell_type":"markdown","metadata":{"id":"M07MPnxl6fcG"},"source":["# 설정파일 설정 및 데이터셋 준비"]},{"cell_type":"markdown","metadata":{"id":"TErZJNXH6fcG"},"source":["# Label Map 생성"]},{"cell_type":"code","metadata":{"id":"KLRzT0Aq6fcI","executionInfo":{"status":"ok","timestamp":1632891843391,"user_tz":-540,"elapsed":255,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["names = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n","# len(names), names\n","ids = range(1, 27)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJpKWa5x6fcJ","executionInfo":{"status":"ok","timestamp":1632891844552,"user_tz":-540,"elapsed":257,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["# File IO를 이용해 label map 파일을 출력\n","with open(LABEL_MAP_FILE_PATH, 'wt') as fw:\n","    for id, name in zip(ids, names):\n","        fw.write('item {\\n')\n","        fw.write(f'\\tid:{id}\\n')\n","        fw.write(f\"\\tname:'{name}'\\n\")\n","        fw.write('}\\n')"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jr2mkycr6fcK"},"source":["# TFRecord 생성"]},{"cell_type":"code","metadata":{"id":"t-Z42lnADm06","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1632891874261,"user_tz":-540,"elapsed":381,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"daee54be-1a83-47c0-c985-d92f887894da"},"source":["# train set tfrecord 생성 명령어\n","f\"!python {SCRIPTS_PATH}/generate_tfrecord.py -x {IMAGE_PATH}/train -l {LABEL_MAP_FILE_PATH} -o {TF_RECORD_PATH}/train.tfr\""],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!python /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/scripts/generate_tfrecord.py -x /content/images/train -l /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/labelmap/label_map.pbtxt -o /content/tfrecord/train.tfr'"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"6eQ4fzFvDm06","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632891885559,"user_tz":-540,"elapsed":5536,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"ee43181a-bda0-4f9e-bd23-00c67719d9cd"},"source":["!python /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/scripts/generate_tfrecord.py -x /content/images/train -l /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/labelmap/label_map.pbtxt -o /content/tfrecord/train.tfr"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully created the TFRecord file: /content/tfrecord/train.tfr\n"]}]},{"cell_type":"code","metadata":{"id":"CjtOvx5bDm06","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1632891887774,"user_tz":-540,"elapsed":387,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"5417edb2-7e16-4d46-c24a-b50b26060ba2"},"source":["# validation set 생성\n","f\"!python {SCRIPTS_PATH}/generate_tfrecord.py -x {IMAGE_PATH}/valid -l {LABEL_MAP_FILE_PATH} -o {TF_RECORD_PATH}/valid.tfr\""],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!python /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/scripts/generate_tfrecord.py -x /content/images/valid -l /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/labelmap/label_map.pbtxt -o /content/tfrecord/valid.tfr'"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GL2EOwdvJqFy","executionInfo":{"status":"ok","timestamp":1632891908620,"user_tz":-540,"elapsed":3251,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"9295055c-5fb2-4070-d53e-c00e37e86a97"},"source":["!python /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/scripts/generate_tfrecord.py -x /content/images/valid -l /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/labelmap/label_map.pbtxt -o /content/tfrecord/valid.tfr"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully created the TFRecord file: /content/tfrecord/valid.tfr\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"UZcZq8s0JrX1","executionInfo":{"status":"ok","timestamp":1632891909667,"user_tz":-540,"elapsed":10,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"dfb2e35b-a806-4e15-fc38-cc93fb5cdbe5"},"source":["# test set 생성\n","f\"!python {SCRIPTS_PATH}/generate_tfrecord.py -x {IMAGE_PATH}/test -l {LABEL_MAP_FILE_PATH} -o {TF_RECORD_PATH}/test.tfr\""],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!python /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/scripts/generate_tfrecord.py -x /content/images/test -l /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/labelmap/label_map.pbtxt -o /content/tfrecord/test.tfr'"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"jc06QvBIDm07","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632891923330,"user_tz":-540,"elapsed":3280,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"9d8ac8eb-187b-407e-b483-be3d843f566f"},"source":["!python /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/scripts/generate_tfrecord.py -x /content/images/test -l /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/labelmap/label_map.pbtxt -o /content/tfrecord/test.tfr"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully created the TFRecord file: /content/tfrecord/test.tfr\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"MuugGtcUKpPi","executionInfo":{"status":"ok","timestamp":1632891927509,"user_tz":-540,"elapsed":239,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"991af049-e2d1-46bc-814e-499ad700eaea"},"source":["# 생성된 tfrecord 파일을 google drive에 카피(백업)\n","f\"!cp {TF_RECORD_PATH}/*.tfr {os.path.join(BASE_PATH, 'tfrecord')}\""],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!cp /content/tfrecord/*.tfr /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/tfrecord'"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"kUCtO-YhKpNP","executionInfo":{"status":"ok","timestamp":1632891932087,"user_tz":-540,"elapsed":377,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["!cp /content/tfrecord/*.tfr /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/tfrecord"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lq69yBSD6fcM"},"source":["# Pretrained Model Download\n","- Tensorflow object detection API는 MS COCO 2017 dataset으로 미리 학습시킨 다양한 Object Detection 모델을 제공한다.\n","- tf2 detection Model Zoo: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n","- SSD MobileNet V2 FPNLite 320x320 다운로드\n","    - 성능은 떨어지지만 학습속도가 빠르다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_-9h7r26fcN","executionInfo":{"status":"ok","timestamp":1632891958984,"user_tz":-540,"elapsed":248,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"3e0e1a29-638a-495e-b436-8ba0d90cf3e0"},"source":["# 리눅스 명령어 wget url: url의 파일을 다운로드하는 리눅스 명령어\n","!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-09-29 05:05:56--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n","Resolving download.tensorflow.org (download.tensorflow.org)... 172.217.214.128, 2607:f8b0:4001:c05::80\n","Connecting to download.tensorflow.org (download.tensorflow.org)|172.217.214.128|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 20515344 (20M) [application/x-tar]\n","Saving to: ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz’\n","\n","\r          ssd_mobil   0%[                    ]       0  --.-KB/s               \rssd_mobilenet_v2_fp 100%[===================>]  19.56M  --.-KB/s    in 0.08s   \n","\n","2021-09-29 05:05:56 (240 MB/s) - ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz’ saved [20515344/20515344]\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"zsqqoOR36fcN","executionInfo":{"status":"ok","timestamp":1632891970108,"user_tz":-540,"elapsed":298,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"9fef51f9-1502-4b53-909e-4ec68eba27f1"},"source":["# 다운받은 모델을 google drive workspace pretrained model로 옮기고 압축 풀기\n","# !mv 원본경로 타겟경로\n","f\"!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz {PRE_TRAINED_MODEL_PATH}\""],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/pre_trained_model'"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"aI1YMYNw6fcN","executionInfo":{"status":"ok","timestamp":1632891981544,"user_tz":-540,"elapsed":269,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/pre_trained_model"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"Q2QGdqyR8kxW","executionInfo":{"status":"ok","timestamp":1632891989066,"user_tz":-540,"elapsed":339,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"37514abc-1187-44ae-f0de-bf6edc97b626"},"source":["# 압축 풀기\n","# tar.gz : !tar -zxvf 압축파일경로 -C 압축풀경로\n","f\"!tar -zxvf {PRE_TRAINED_MODEL_PATH}/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz -C {PRE_TRAINED_MODEL_PATH}\""],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!tar -zxvf /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/pre_trained_model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz -C /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/pre_trained_model'"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-d5EPJLQ8mig","executionInfo":{"status":"ok","timestamp":1632891994795,"user_tz":-540,"elapsed":912,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"56b919fe-42f7-4c00-a4f5-cc69b3c377a9"},"source":["!tar -zxvf /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/pre_trained_model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz -C /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/pre_trained_model"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/checkpoint\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0.index\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/saved_model.pb\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/variables.index\n"]}]},{"cell_type":"markdown","metadata":{"id":"h1p0r3II6fcN"},"source":["# Pipeline.config 설정 변경"]},{"cell_type":"markdown","metadata":{"id":"VlXp-kA_6fcO"},"source":["## pipeline.config  파일 개요\n","- Model을 학습, 검증하기 위해 필요한 설정을 하는 파일\n","- 구조\n","    - https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md\n","    - **model**\n","        - 사용하는 모델에 대한 설정\n","        - class 개수\n","        - 입력이미지 size\n","        - anchor 설정\n","    - **train_config**\n","        - Train(학습)관련 설정\n","        - batch_size\n","            - 사용하는 GPU의 메모리 크기에 맞게 조절한다.\n","        - image augmentation관련 설정 등\n","        - optimizer관련 설정\n","        - 학습에 사용할 weight 파일의 경로\n","    - **train_input_reader**\n","        - labelmap 파일 경로\n","        - train tfrecord 파일 경로\n","    - **eval_config**\n","        - evaluation(평가)을 위해 사용하는 metric 설정\n","    - **eval_input_reader**\n","        - labelmap 파일 경로\n","        - evaluation tfreord 파일 경로\n","        "]},{"cell_type":"markdown","metadata":{"id":"fv9VRU7Y6fcO"},"source":["## Pretrain model의 pipeline.config 파일 카피\n","- pretrained 모델의 압축을 풀면 pipeline.config 파일이 있다.\n","- workspace\\model 로 copy 한다."]},{"cell_type":"code","metadata":{"id":"-gP4dmZf6fcO","executionInfo":{"status":"ok","timestamp":1632892014218,"user_tz":-540,"elapsed":326,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["PRE_TRAINED_MODEL_PATH = os.path.join(PRE_TRAINED_MODEL_PATH, \"ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\")"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"XsYbDMcI8svQ","executionInfo":{"status":"ok","timestamp":1632892023292,"user_tz":-540,"elapsed":266,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"4225ccf0-4d1e-4cd1-f445-ae1b5d6567db"},"source":["f\"!cp {os.path.join(PRE_TRAINED_MODEL_PATH, 'pipeline.config')} {PIPELINE_CONFIG_PATH}\""],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!cp /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/pre_trained_model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/pipeline.config'"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"rIi-Lt2E6fcO","executionInfo":{"status":"ok","timestamp":1632892028433,"user_tz":-540,"elapsed":270,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["!cp /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/pre_trained_model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/pipeline.config"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJAsq7EN6fcO"},"source":["## pipeline.config 설정 변경\n","- pipeline.config 내용 변경은 파일을 **직접 변경**할 수도 있고 **코드상에서 변경**할 수도 있다.\n","\n","### 필수 변경사항\n","-  class개수 변경\n","-  train 배치 사이즈 변경 - gpu 메모리 사양에 맞게 변경한다.\n","-  pretrained model 경로 설정\n","-  pretrained model이 어떤 종류의 모델인지 설정\n","-  train 관련 변경\n","    -  labelmap 파일 경로 설정\n","    -  train 용 tfrecord 파일 경로 지정\n","-  evaluation 관련 변경\n","    -  labelmap 파일 경로 설정\n","    -  evaluation 용 tfrecord 파일 경로 지정"]},{"cell_type":"code","metadata":{"id":"4EPSTPbp6fcP","executionInfo":{"status":"ok","timestamp":1632892045561,"user_tz":-540,"elapsed":2021,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["# Tensorflow Object Detection API에서 제공하는 Library를 이용해 pipeline.config 변환 작업\n","import tensorflow as tf\n","from object_detection.utils import config_util\n","from object_detection.protos import pipeline_pb2\n","from google.protobuf import text_format"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Y1s5hpu6fcP","executionInfo":{"status":"ok","timestamp":1632892050686,"user_tz":-540,"elapsed":255,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"d0827c3f-d440-4deb-d6be-d47cd9d00d00"},"source":["# pipeline.config 파일을 읽어서 확인\n","conf = config_util.get_configs_from_pipeline_file(PIPELINE_CONFIG_PATH)   # 받은 경로의 pipeline.config파일의 설정을 딕셔너리로 읽어온다.\n","print(type(conf))\n","conf"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'dict'>\n"]},{"output_type":"execute_result","data":{"text/plain":["{'eval_config': metrics_set: \"coco_detection_metrics\"\n"," use_moving_averages: false,\n"," 'eval_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\n"," shuffle: false\n"," num_epochs: 1\n"," tf_record_input_reader {\n","   input_path: \"PATH_TO_BE_CONFIGURED\"\n"," },\n"," 'eval_input_configs': [label_map_path: \"PATH_TO_BE_CONFIGURED\"\n"," shuffle: false\n"," num_epochs: 1\n"," tf_record_input_reader {\n","   input_path: \"PATH_TO_BE_CONFIGURED\"\n"," }\n"," ],\n"," 'model': ssd {\n","   num_classes: 90\n","   image_resizer {\n","     fixed_shape_resizer {\n","       height: 320\n","       width: 320\n","     }\n","   }\n","   feature_extractor {\n","     type: \"ssd_mobilenet_v2_fpn_keras\"\n","     depth_multiplier: 1.0\n","     min_depth: 16\n","     conv_hyperparams {\n","       regularizer {\n","         l2_regularizer {\n","           weight: 3.9999998989515007e-05\n","         }\n","       }\n","       initializer {\n","         random_normal_initializer {\n","           mean: 0.0\n","           stddev: 0.009999999776482582\n","         }\n","       }\n","       activation: RELU_6\n","       batch_norm {\n","         decay: 0.996999979019165\n","         scale: true\n","         epsilon: 0.0010000000474974513\n","       }\n","     }\n","     use_depthwise: true\n","     override_base_feature_extractor_hyperparams: true\n","     fpn {\n","       min_level: 3\n","       max_level: 7\n","       additional_layer_depth: 128\n","     }\n","   }\n","   box_coder {\n","     faster_rcnn_box_coder {\n","       y_scale: 10.0\n","       x_scale: 10.0\n","       height_scale: 5.0\n","       width_scale: 5.0\n","     }\n","   }\n","   matcher {\n","     argmax_matcher {\n","       matched_threshold: 0.5\n","       unmatched_threshold: 0.5\n","       ignore_thresholds: false\n","       negatives_lower_than_unmatched: true\n","       force_match_for_each_row: true\n","       use_matmul_gather: true\n","     }\n","   }\n","   similarity_calculator {\n","     iou_similarity {\n","     }\n","   }\n","   box_predictor {\n","     weight_shared_convolutional_box_predictor {\n","       conv_hyperparams {\n","         regularizer {\n","           l2_regularizer {\n","             weight: 3.9999998989515007e-05\n","           }\n","         }\n","         initializer {\n","           random_normal_initializer {\n","             mean: 0.0\n","             stddev: 0.009999999776482582\n","           }\n","         }\n","         activation: RELU_6\n","         batch_norm {\n","           decay: 0.996999979019165\n","           scale: true\n","           epsilon: 0.0010000000474974513\n","         }\n","       }\n","       depth: 128\n","       num_layers_before_predictor: 4\n","       kernel_size: 3\n","       class_prediction_bias_init: -4.599999904632568\n","       share_prediction_tower: true\n","       use_depthwise: true\n","     }\n","   }\n","   anchor_generator {\n","     multiscale_anchor_generator {\n","       min_level: 3\n","       max_level: 7\n","       anchor_scale: 4.0\n","       aspect_ratios: 1.0\n","       aspect_ratios: 2.0\n","       aspect_ratios: 0.5\n","       scales_per_octave: 2\n","     }\n","   }\n","   post_processing {\n","     batch_non_max_suppression {\n","       score_threshold: 9.99999993922529e-09\n","       iou_threshold: 0.6000000238418579\n","       max_detections_per_class: 100\n","       max_total_detections: 100\n","       use_static_shapes: false\n","     }\n","     score_converter: SIGMOID\n","   }\n","   normalize_loss_by_num_matches: true\n","   loss {\n","     localization_loss {\n","       weighted_smooth_l1 {\n","       }\n","     }\n","     classification_loss {\n","       weighted_sigmoid_focal {\n","         gamma: 2.0\n","         alpha: 0.25\n","       }\n","     }\n","     classification_weight: 1.0\n","     localization_weight: 1.0\n","   }\n","   encode_background_as_zeros: true\n","   normalize_loc_loss_by_codesize: true\n","   inplace_batchnorm_update: true\n","   freeze_batchnorm: false\n"," },\n"," 'train_config': batch_size: 128\n"," data_augmentation_options {\n","   random_horizontal_flip {\n","   }\n"," }\n"," data_augmentation_options {\n","   random_crop_image {\n","     min_object_covered: 0.0\n","     min_aspect_ratio: 0.75\n","     max_aspect_ratio: 3.0\n","     min_area: 0.75\n","     max_area: 1.0\n","     overlap_thresh: 0.0\n","   }\n"," }\n"," sync_replicas: true\n"," optimizer {\n","   momentum_optimizer {\n","     learning_rate {\n","       cosine_decay_learning_rate {\n","         learning_rate_base: 0.07999999821186066\n","         total_steps: 50000\n","         warmup_learning_rate: 0.026666000485420227\n","         warmup_steps: 1000\n","       }\n","     }\n","     momentum_optimizer_value: 0.8999999761581421\n","   }\n","   use_moving_average: false\n"," }\n"," fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED\"\n"," num_steps: 50000\n"," startup_delay_steps: 0.0\n"," replicas_to_aggregate: 8\n"," max_number_of_boxes: 100\n"," unpad_groundtruth_tensors: false\n"," fine_tune_checkpoint_type: \"classification\"\n"," fine_tune_checkpoint_version: V2,\n"," 'train_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\n"," tf_record_input_reader {\n","   input_path: \"PATH_TO_BE_CONFIGURED\"\n"," }}"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5UJ2dDBM6fcP","executionInfo":{"status":"ok","timestamp":1632892067996,"user_tz":-540,"elapsed":276,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"300cf463-a207-4290-a442-c31c1b19d29d"},"source":["# 수정 작업\n","# 빈 pipeline.config 템플릿을 생성\n","pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()   # pipeline.config의 속성들을 수정하는 기능을 제공\n","print(type(pipeline_config))\n","print(pipeline_config)"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'object_detection.protos.pipeline_pb2.TrainEvalPipelineConfig'>\n","\n"]}]},{"cell_type":"code","metadata":{"id":"uCDhk84q853I","executionInfo":{"status":"ok","timestamp":1632892073204,"user_tz":-540,"elapsed":249,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["# pipeline.config의 내용을 text로 읽어서 TrainEvalPipelineConfig(템플릿-틀)에 넣는다.\n","with tf.io.gfile.GFile(PIPELINE_CONFIG_PATH, 'r') as fr:  # open()의 Tensorflow 버전\n","    proto_str = fr.read()   # text로 읽기\n","    text_format.Merge(proto_str, pipeline_config)   # 읽은 설정 text를 속성단위로 나눠서 TrainEvalPipelineConfig에 넣어준다."],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzGjPPX16fcQ","executionInfo":{"status":"ok","timestamp":1632892101387,"user_tz":-540,"elapsed":242,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["# 각 항목(속성)들을 변환\n","pipeline_config.model.ssd.num_classes = 26   # 검출할 class 개수\n","pipeline_config.train_config.batch_size = 8\n","pipeline_config.train_config.fine_tune_checkpoint = os.path.join(PRE_TRAINED_MODEL_PATH, 'checkpoint', 'ckpt-0')   # pretrained model의 weiht(전이학습에서 초기 weight - 확장자 뺀 파일명 주기)\n","pipeline_config.train_config.fine_tune_checkpoint_type = 'detection'    # pretrained model이 어떤 작업을 위한 학습한 모델인지 지정 (object detection=detection)\n","\n","# train dataset 관련 설정\n","# label map 경로\n","pipeline_config.train_input_reader.label_map_path = LABEL_MAP_FILE_PATH\n","# tfrecord 경로\n","pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(TF_RECORD_PATH, 'train.tfr')]\n","# evaluation dataset 관련 설정\n","# label map 경로\n","pipeline_config.eval_input_reader[0].label_map_path = LABEL_MAP_FILE_PATH\n","# tfrecord 경로\n","pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(TF_RECORD_PATH, 'valid.tfr')]"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"liy_Dt0H6fcQ","executionInfo":{"status":"ok","timestamp":1632892124788,"user_tz":-540,"elapsed":314,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"d1928057-0dcd-4229-992a-10542a32cf03"},"source":["print(pipeline_config)"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["model {\n","  ssd {\n","    num_classes: 26\n","    image_resizer {\n","      fixed_shape_resizer {\n","        height: 320\n","        width: 320\n","      }\n","    }\n","    feature_extractor {\n","      type: \"ssd_mobilenet_v2_fpn_keras\"\n","      depth_multiplier: 1.0\n","      min_depth: 16\n","      conv_hyperparams {\n","        regularizer {\n","          l2_regularizer {\n","            weight: 3.9999998989515007e-05\n","          }\n","        }\n","        initializer {\n","          random_normal_initializer {\n","            mean: 0.0\n","            stddev: 0.009999999776482582\n","          }\n","        }\n","        activation: RELU_6\n","        batch_norm {\n","          decay: 0.996999979019165\n","          scale: true\n","          epsilon: 0.0010000000474974513\n","        }\n","      }\n","      use_depthwise: true\n","      override_base_feature_extractor_hyperparams: true\n","      fpn {\n","        min_level: 3\n","        max_level: 7\n","        additional_layer_depth: 128\n","      }\n","    }\n","    box_coder {\n","      faster_rcnn_box_coder {\n","        y_scale: 10.0\n","        x_scale: 10.0\n","        height_scale: 5.0\n","        width_scale: 5.0\n","      }\n","    }\n","    matcher {\n","      argmax_matcher {\n","        matched_threshold: 0.5\n","        unmatched_threshold: 0.5\n","        ignore_thresholds: false\n","        negatives_lower_than_unmatched: true\n","        force_match_for_each_row: true\n","        use_matmul_gather: true\n","      }\n","    }\n","    similarity_calculator {\n","      iou_similarity {\n","      }\n","    }\n","    box_predictor {\n","      weight_shared_convolutional_box_predictor {\n","        conv_hyperparams {\n","          regularizer {\n","            l2_regularizer {\n","              weight: 3.9999998989515007e-05\n","            }\n","          }\n","          initializer {\n","            random_normal_initializer {\n","              mean: 0.0\n","              stddev: 0.009999999776482582\n","            }\n","          }\n","          activation: RELU_6\n","          batch_norm {\n","            decay: 0.996999979019165\n","            scale: true\n","            epsilon: 0.0010000000474974513\n","          }\n","        }\n","        depth: 128\n","        num_layers_before_predictor: 4\n","        kernel_size: 3\n","        class_prediction_bias_init: -4.599999904632568\n","        share_prediction_tower: true\n","        use_depthwise: true\n","      }\n","    }\n","    anchor_generator {\n","      multiscale_anchor_generator {\n","        min_level: 3\n","        max_level: 7\n","        anchor_scale: 4.0\n","        aspect_ratios: 1.0\n","        aspect_ratios: 2.0\n","        aspect_ratios: 0.5\n","        scales_per_octave: 2\n","      }\n","    }\n","    post_processing {\n","      batch_non_max_suppression {\n","        score_threshold: 9.99999993922529e-09\n","        iou_threshold: 0.6000000238418579\n","        max_detections_per_class: 100\n","        max_total_detections: 100\n","        use_static_shapes: false\n","      }\n","      score_converter: SIGMOID\n","    }\n","    normalize_loss_by_num_matches: true\n","    loss {\n","      localization_loss {\n","        weighted_smooth_l1 {\n","        }\n","      }\n","      classification_loss {\n","        weighted_sigmoid_focal {\n","          gamma: 2.0\n","          alpha: 0.25\n","        }\n","      }\n","      classification_weight: 1.0\n","      localization_weight: 1.0\n","    }\n","    encode_background_as_zeros: true\n","    normalize_loc_loss_by_codesize: true\n","    inplace_batchnorm_update: true\n","    freeze_batchnorm: false\n","  }\n","}\n","train_config {\n","  batch_size: 8\n","  data_augmentation_options {\n","    random_horizontal_flip {\n","    }\n","  }\n","  data_augmentation_options {\n","    random_crop_image {\n","      min_object_covered: 0.0\n","      min_aspect_ratio: 0.75\n","      max_aspect_ratio: 3.0\n","      min_area: 0.75\n","      max_area: 1.0\n","      overlap_thresh: 0.0\n","    }\n","  }\n","  sync_replicas: true\n","  optimizer {\n","    momentum_optimizer {\n","      learning_rate {\n","        cosine_decay_learning_rate {\n","          learning_rate_base: 0.07999999821186066\n","          total_steps: 50000\n","          warmup_learning_rate: 0.026666000485420227\n","          warmup_steps: 1000\n","        }\n","      }\n","      momentum_optimizer_value: 0.8999999761581421\n","    }\n","    use_moving_average: false\n","  }\n","  fine_tune_checkpoint: \"/content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/pre_trained_model/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n","  num_steps: 50000\n","  startup_delay_steps: 0.0\n","  replicas_to_aggregate: 8\n","  max_number_of_boxes: 100\n","  unpad_groundtruth_tensors: false\n","  fine_tune_checkpoint_type: \"detection\"\n","  fine_tune_checkpoint_version: V2\n","}\n","train_input_reader {\n","  label_map_path: \"/content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/labelmap/label_map.pbtxt\"\n","  tf_record_input_reader {\n","    input_path: \"/content/tfrecord/train.tfr\"\n","  }\n","}\n","eval_config {\n","  metrics_set: \"coco_detection_metrics\"\n","  use_moving_averages: false\n","}\n","eval_input_reader {\n","  label_map_path: \"/content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/labelmap/label_map.pbtxt\"\n","  shuffle: false\n","  num_epochs: 1\n","  tf_record_input_reader {\n","    input_path: \"/content/tfrecord/valid.tfr\"\n","  }\n","}\n","\n"]}]},{"cell_type":"code","metadata":{"id":"JUGuKD1m6fcQ","executionInfo":{"status":"ok","timestamp":1632892148304,"user_tz":-540,"elapsed":248,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["# 변경된 내용을 pipeline.config 파일에 덮어쓰기\n","# TrainEvalPipelineConfig를 text(str)으로 변환\n","config_text = text_format.MessageToString(pipeline_config)\n","# 파일로 저장(출력)\n","with open(PIPELINE_CONFIG_PATH, 'w') as fw:\n","    fw.write(config_text)"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1GW73RDb6fcQ"},"source":["# Model 학습\n","- 다음 명령어를 실행한다.\n","- 시간이 오래 걸리므로 terminal에서 실행한다.\n","```\n","python models/research/object_detection/model_main_tf2.py --model_dir=workspace/model/checkpoint --pipeline_config_path=workspace/model/pipeline.config --num_train_steps=3000\n","```\n","\n","## 옵션\n","- model_dir: 학습한 모델의 checkpoint 파일을 저장할 경로. (1000 step당 저장한다.)\n","- pipeline_config_path: pipeline.config 파일 경로\n","- num_train_steps: 학습할 step 수"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"s_PrOim96fcQ","executionInfo":{"status":"ok","timestamp":1632892161149,"user_tz":-540,"elapsed":265,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"d58c4c71-9825-470b-ad73-7692dccb6343"},"source":["f\"!python models/research/object_detection/model_main_tf2.py --model_dir {CHECK_POINT_PATH} --pipeline_config_path {PIPELINE_CONFIG_PATH} --num_train_steps=3000\""],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!python models/research/object_detection/model_main_tf2.py --model_dir /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/checkpoint --pipeline_config_path /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/pipeline.config --num_train_steps=3000'"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ENcmksZB6fcQ","executionInfo":{"status":"ok","timestamp":1632893276089,"user_tz":-540,"elapsed":1109858,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"54101d2a-3afb-459f-96fa-cccb653ad3e8"},"source":["# !python models/research/object_detection/model_main_tf2.py --model_dir /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/checkpoint --pipeline_config_path /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/pipeline.config --num_train_steps=3000"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-09-29 05:09:26.933212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:09:26.942655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:09:26.943456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:09:26.945050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:09:26.945860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:09:26.946641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:09:27.425193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:09:27.426119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:09:27.427040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:09:27.427994: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2021-09-29 05:09:27.428115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10819 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n","INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","I0929 05:09:27.433277 140223081744256 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","INFO:tensorflow:Maybe overwriting train_steps: 3000\n","I0929 05:09:27.439444 140223081744256 config_util.py:552] Maybe overwriting train_steps: 3000\n","INFO:tensorflow:Maybe overwriting use_bfloat16: False\n","I0929 05:09:27.439657 140223081744256 config_util.py:552] Maybe overwriting use_bfloat16: False\n","WARNING:tensorflow:From /content/models/research/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","rename to distribute_datasets_from_function\n","W0929 05:09:27.579188 140223081744256 deprecation.py:345] From /content/models/research/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","rename to distribute_datasets_from_function\n","INFO:tensorflow:Reading unweighted datasets: ['/content/tfrecord/train.tfr']\n","I0929 05:09:27.611332 140223081744256 dataset_builder.py:163] Reading unweighted datasets: ['/content/tfrecord/train.tfr']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/tfrecord/train.tfr']\n","I0929 05:09:27.611622 140223081744256 dataset_builder.py:80] Reading record datasets for input file: ['/content/tfrecord/train.tfr']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0929 05:09:27.611749 140223081744256 dataset_builder.py:81] Number of filenames to read: 1\n","WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n","W0929 05:09:27.611902 140223081744256 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n","WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n","W0929 05:09:27.624642 140223081744256 deprecation.py:345] From /content/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n","WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","W0929 05:09:27.651698 140223081744256 deprecation.py:345] From /content/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","W0929 05:09:35.679861 140223081744256 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n","W0929 05:09:39.537135 140223081744256 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0929 05:09:41.579484 140223081744256 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","2021-09-29 05:09:44.490099: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","/usr/local/lib/python3.7/dist-packages/keras/backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n","  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n","2021-09-29 05:10:11.178009: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0929 05:10:41.031359 140223081744256 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0929 05:10:41.032603 140223081744256 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0929 05:10:41.035160 140223081744256 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0929 05:10:41.036299 140223081744256 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0929 05:10:41.038844 140223081744256 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0929 05:10:41.039900 140223081744256 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0929 05:10:41.042527 140223081744256 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0929 05:10:41.044693 140223081744256 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0929 05:10:41.047153 140223081744256 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I0929 05:10:41.048152 140223081744256 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use fn_output_signature instead\n","W0929 05:10:41.871017 140218913765120 deprecation.py:548] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use fn_output_signature instead\n","INFO:tensorflow:Step 100 per-step time 0.750s\n","I0929 05:11:56.405376 140223081744256 model_lib_v2.py:700] Step 100 per-step time 0.750s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8290557,\n"," 'Loss/localization_loss': 0.19157031,\n"," 'Loss/regularization_loss': 0.15345983,\n"," 'Loss/total_loss': 1.1740859,\n"," 'learning_rate': 0.0319994}\n","I0929 05:11:56.405831 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.8290557,\n"," 'Loss/localization_loss': 0.19157031,\n"," 'Loss/regularization_loss': 0.15345983,\n"," 'Loss/total_loss': 1.1740859,\n"," 'learning_rate': 0.0319994}\n","INFO:tensorflow:Step 200 per-step time 0.330s\n","I0929 05:12:29.367626 140223081744256 model_lib_v2.py:700] Step 200 per-step time 0.330s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6058341,\n"," 'Loss/localization_loss': 0.15273173,\n"," 'Loss/regularization_loss': 0.15354213,\n"," 'Loss/total_loss': 0.91210794,\n"," 'learning_rate': 0.0373328}\n","I0929 05:12:29.368051 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.6058341,\n"," 'Loss/localization_loss': 0.15273173,\n"," 'Loss/regularization_loss': 0.15354213,\n"," 'Loss/total_loss': 0.91210794,\n"," 'learning_rate': 0.0373328}\n","INFO:tensorflow:Step 300 per-step time 0.329s\n","I0929 05:13:02.224863 140223081744256 model_lib_v2.py:700] Step 300 per-step time 0.329s\n","INFO:tensorflow:{'Loss/classification_loss': 0.49451375,\n"," 'Loss/localization_loss': 0.092640735,\n"," 'Loss/regularization_loss': 0.15348741,\n"," 'Loss/total_loss': 0.74064195,\n"," 'learning_rate': 0.0426662}\n","I0929 05:13:02.225207 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.49451375,\n"," 'Loss/localization_loss': 0.092640735,\n"," 'Loss/regularization_loss': 0.15348741,\n"," 'Loss/total_loss': 0.74064195,\n"," 'learning_rate': 0.0426662}\n","INFO:tensorflow:Step 400 per-step time 0.332s\n","I0929 05:13:35.395725 140223081744256 model_lib_v2.py:700] Step 400 per-step time 0.332s\n","INFO:tensorflow:{'Loss/classification_loss': 0.40882522,\n"," 'Loss/localization_loss': 0.09489293,\n"," 'Loss/regularization_loss': 0.15331107,\n"," 'Loss/total_loss': 0.6570292,\n"," 'learning_rate': 0.047999598}\n","I0929 05:13:35.396083 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.40882522,\n"," 'Loss/localization_loss': 0.09489293,\n"," 'Loss/regularization_loss': 0.15331107,\n"," 'Loss/total_loss': 0.6570292,\n"," 'learning_rate': 0.047999598}\n","INFO:tensorflow:Step 500 per-step time 0.330s\n","I0929 05:14:08.377494 140223081744256 model_lib_v2.py:700] Step 500 per-step time 0.330s\n","INFO:tensorflow:{'Loss/classification_loss': 0.49831128,\n"," 'Loss/localization_loss': 0.07272734,\n"," 'Loss/regularization_loss': 0.15319932,\n"," 'Loss/total_loss': 0.7242379,\n"," 'learning_rate': 0.053333}\n","I0929 05:14:08.377909 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.49831128,\n"," 'Loss/localization_loss': 0.07272734,\n"," 'Loss/regularization_loss': 0.15319932,\n"," 'Loss/total_loss': 0.7242379,\n"," 'learning_rate': 0.053333}\n","INFO:tensorflow:Step 600 per-step time 0.328s\n","I0929 05:14:41.213314 140223081744256 model_lib_v2.py:700] Step 600 per-step time 0.328s\n","INFO:tensorflow:{'Loss/classification_loss': 0.44581735,\n"," 'Loss/localization_loss': 0.07953422,\n"," 'Loss/regularization_loss': 0.15298907,\n"," 'Loss/total_loss': 0.6783407,\n"," 'learning_rate': 0.0586664}\n","I0929 05:14:41.213692 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.44581735,\n"," 'Loss/localization_loss': 0.07953422,\n"," 'Loss/regularization_loss': 0.15298907,\n"," 'Loss/total_loss': 0.6783407,\n"," 'learning_rate': 0.0586664}\n","INFO:tensorflow:Step 700 per-step time 0.331s\n","I0929 05:15:14.290459 140223081744256 model_lib_v2.py:700] Step 700 per-step time 0.331s\n","INFO:tensorflow:{'Loss/classification_loss': 0.4322124,\n"," 'Loss/localization_loss': 0.09530723,\n"," 'Loss/regularization_loss': 0.15317625,\n"," 'Loss/total_loss': 0.6806959,\n"," 'learning_rate': 0.0639998}\n","I0929 05:15:14.290796 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.4322124,\n"," 'Loss/localization_loss': 0.09530723,\n"," 'Loss/regularization_loss': 0.15317625,\n"," 'Loss/total_loss': 0.6806959,\n"," 'learning_rate': 0.0639998}\n","INFO:tensorflow:Step 800 per-step time 0.330s\n","I0929 05:15:47.250254 140223081744256 model_lib_v2.py:700] Step 800 per-step time 0.330s\n","INFO:tensorflow:{'Loss/classification_loss': 0.39337602,\n"," 'Loss/localization_loss': 0.062850334,\n"," 'Loss/regularization_loss': 0.1530397,\n"," 'Loss/total_loss': 0.60926604,\n"," 'learning_rate': 0.069333196}\n","I0929 05:15:47.250631 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.39337602,\n"," 'Loss/localization_loss': 0.062850334,\n"," 'Loss/regularization_loss': 0.1530397,\n"," 'Loss/total_loss': 0.60926604,\n"," 'learning_rate': 0.069333196}\n","INFO:tensorflow:Step 900 per-step time 0.329s\n","I0929 05:16:20.199308 140223081744256 model_lib_v2.py:700] Step 900 per-step time 0.329s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2873394,\n"," 'Loss/localization_loss': 0.081475616,\n"," 'Loss/regularization_loss': 0.15266596,\n"," 'Loss/total_loss': 0.521481,\n"," 'learning_rate': 0.074666604}\n","I0929 05:16:20.199687 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.2873394,\n"," 'Loss/localization_loss': 0.081475616,\n"," 'Loss/regularization_loss': 0.15266596,\n"," 'Loss/total_loss': 0.521481,\n"," 'learning_rate': 0.074666604}\n","INFO:tensorflow:Step 1000 per-step time 0.331s\n","I0929 05:16:53.287301 140223081744256 model_lib_v2.py:700] Step 1000 per-step time 0.331s\n","INFO:tensorflow:{'Loss/classification_loss': 0.33806908,\n"," 'Loss/localization_loss': 0.075342156,\n"," 'Loss/regularization_loss': 0.15242597,\n"," 'Loss/total_loss': 0.5658372,\n"," 'learning_rate': 0.08}\n","I0929 05:16:53.287663 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.33806908,\n"," 'Loss/localization_loss': 0.075342156,\n"," 'Loss/regularization_loss': 0.15242597,\n"," 'Loss/total_loss': 0.5658372,\n"," 'learning_rate': 0.08}\n","INFO:tensorflow:Step 1100 per-step time 0.339s\n","I0929 05:17:27.213321 140223081744256 model_lib_v2.py:700] Step 1100 per-step time 0.339s\n","INFO:tensorflow:{'Loss/classification_loss': 0.4609134,\n"," 'Loss/localization_loss': 0.0670947,\n"," 'Loss/regularization_loss': 0.15318699,\n"," 'Loss/total_loss': 0.6811951,\n"," 'learning_rate': 0.07999918}\n","I0929 05:17:27.213697 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.4609134,\n"," 'Loss/localization_loss': 0.0670947,\n"," 'Loss/regularization_loss': 0.15318699,\n"," 'Loss/total_loss': 0.6811951,\n"," 'learning_rate': 0.07999918}\n","INFO:tensorflow:Step 1200 per-step time 0.329s\n","I0929 05:18:00.125404 140223081744256 model_lib_v2.py:700] Step 1200 per-step time 0.329s\n","INFO:tensorflow:{'Loss/classification_loss': 0.31478575,\n"," 'Loss/localization_loss': 0.050961,\n"," 'Loss/regularization_loss': 0.15318097,\n"," 'Loss/total_loss': 0.5189277,\n"," 'learning_rate': 0.079996705}\n","I0929 05:18:00.125903 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.31478575,\n"," 'Loss/localization_loss': 0.050961,\n"," 'Loss/regularization_loss': 0.15318097,\n"," 'Loss/total_loss': 0.5189277,\n"," 'learning_rate': 0.079996705}\n","INFO:tensorflow:Step 1300 per-step time 0.330s\n","I0929 05:18:33.100396 140223081744256 model_lib_v2.py:700] Step 1300 per-step time 0.330s\n","INFO:tensorflow:{'Loss/classification_loss': 0.36407086,\n"," 'Loss/localization_loss': 0.039656278,\n"," 'Loss/regularization_loss': 0.15288648,\n"," 'Loss/total_loss': 0.5566136,\n"," 'learning_rate': 0.0799926}\n","I0929 05:18:33.100792 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.36407086,\n"," 'Loss/localization_loss': 0.039656278,\n"," 'Loss/regularization_loss': 0.15288648,\n"," 'Loss/total_loss': 0.5566136,\n"," 'learning_rate': 0.0799926}\n","INFO:tensorflow:Step 1400 per-step time 0.330s\n","I0929 05:19:06.125234 140223081744256 model_lib_v2.py:700] Step 1400 per-step time 0.330s\n","INFO:tensorflow:{'Loss/classification_loss': 0.24513452,\n"," 'Loss/localization_loss': 0.07394632,\n"," 'Loss/regularization_loss': 0.15266858,\n"," 'Loss/total_loss': 0.47174942,\n"," 'learning_rate': 0.07998685}\n","I0929 05:19:06.125603 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.24513452,\n"," 'Loss/localization_loss': 0.07394632,\n"," 'Loss/regularization_loss': 0.15266858,\n"," 'Loss/total_loss': 0.47174942,\n"," 'learning_rate': 0.07998685}\n","INFO:tensorflow:Step 1500 per-step time 0.331s\n","I0929 05:19:39.183896 140223081744256 model_lib_v2.py:700] Step 1500 per-step time 0.331s\n","INFO:tensorflow:{'Loss/classification_loss': 0.22415754,\n"," 'Loss/localization_loss': 0.045760088,\n"," 'Loss/regularization_loss': 0.15228532,\n"," 'Loss/total_loss': 0.42220294,\n"," 'learning_rate': 0.07997945}\n","I0929 05:19:39.184302 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.22415754,\n"," 'Loss/localization_loss': 0.045760088,\n"," 'Loss/regularization_loss': 0.15228532,\n"," 'Loss/total_loss': 0.42220294,\n"," 'learning_rate': 0.07997945}\n","INFO:tensorflow:Step 1600 per-step time 0.328s\n","I0929 05:20:12.010973 140223081744256 model_lib_v2.py:700] Step 1600 per-step time 0.328s\n","INFO:tensorflow:{'Loss/classification_loss': 0.23133844,\n"," 'Loss/localization_loss': 0.059999764,\n"," 'Loss/regularization_loss': 0.1519448,\n"," 'Loss/total_loss': 0.44328302,\n"," 'learning_rate': 0.079970405}\n","I0929 05:20:12.011310 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.23133844,\n"," 'Loss/localization_loss': 0.059999764,\n"," 'Loss/regularization_loss': 0.1519448,\n"," 'Loss/total_loss': 0.44328302,\n"," 'learning_rate': 0.079970405}\n","INFO:tensorflow:Step 1700 per-step time 0.328s\n","I0929 05:20:44.835786 140223081744256 model_lib_v2.py:700] Step 1700 per-step time 0.328s\n","INFO:tensorflow:{'Loss/classification_loss': 0.27372286,\n"," 'Loss/localization_loss': 0.041923348,\n"," 'Loss/regularization_loss': 0.15161254,\n"," 'Loss/total_loss': 0.46725875,\n"," 'learning_rate': 0.07995972}\n","I0929 05:20:44.836159 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.27372286,\n"," 'Loss/localization_loss': 0.041923348,\n"," 'Loss/regularization_loss': 0.15161254,\n"," 'Loss/total_loss': 0.46725875,\n"," 'learning_rate': 0.07995972}\n","INFO:tensorflow:Step 1800 per-step time 0.330s\n","I0929 05:21:17.819912 140223081744256 model_lib_v2.py:700] Step 1800 per-step time 0.330s\n","INFO:tensorflow:{'Loss/classification_loss': 0.21692961,\n"," 'Loss/localization_loss': 0.057508577,\n"," 'Loss/regularization_loss': 0.15119605,\n"," 'Loss/total_loss': 0.42563426,\n"," 'learning_rate': 0.0799474}\n","I0929 05:21:17.820296 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.21692961,\n"," 'Loss/localization_loss': 0.057508577,\n"," 'Loss/regularization_loss': 0.15119605,\n"," 'Loss/total_loss': 0.42563426,\n"," 'learning_rate': 0.0799474}\n","INFO:tensorflow:Step 1900 per-step time 0.326s\n","I0929 05:21:50.399760 140223081744256 model_lib_v2.py:700] Step 1900 per-step time 0.326s\n","INFO:tensorflow:{'Loss/classification_loss': 0.28418094,\n"," 'Loss/localization_loss': 0.05738084,\n"," 'Loss/regularization_loss': 0.15088916,\n"," 'Loss/total_loss': 0.49245095,\n"," 'learning_rate': 0.07993342}\n","I0929 05:21:50.400156 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.28418094,\n"," 'Loss/localization_loss': 0.05738084,\n"," 'Loss/regularization_loss': 0.15088916,\n"," 'Loss/total_loss': 0.49245095,\n"," 'learning_rate': 0.07993342}\n","INFO:tensorflow:Step 2000 per-step time 0.325s\n","I0929 05:22:22.856960 140223081744256 model_lib_v2.py:700] Step 2000 per-step time 0.325s\n","INFO:tensorflow:{'Loss/classification_loss': 0.26738262,\n"," 'Loss/localization_loss': 0.049655482,\n"," 'Loss/regularization_loss': 0.15059519,\n"," 'Loss/total_loss': 0.4676333,\n"," 'learning_rate': 0.07991781}\n","I0929 05:22:22.857344 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.26738262,\n"," 'Loss/localization_loss': 0.049655482,\n"," 'Loss/regularization_loss': 0.15059519,\n"," 'Loss/total_loss': 0.4676333,\n"," 'learning_rate': 0.07991781}\n","INFO:tensorflow:Step 2100 per-step time 0.333s\n","I0929 05:22:56.201728 140223081744256 model_lib_v2.py:700] Step 2100 per-step time 0.333s\n","INFO:tensorflow:{'Loss/classification_loss': 0.14894944,\n"," 'Loss/localization_loss': 0.033587653,\n"," 'Loss/regularization_loss': 0.15021828,\n"," 'Loss/total_loss': 0.3327554,\n"," 'learning_rate': 0.07990056}\n","I0929 05:22:56.202148 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.14894944,\n"," 'Loss/localization_loss': 0.033587653,\n"," 'Loss/regularization_loss': 0.15021828,\n"," 'Loss/total_loss': 0.3327554,\n"," 'learning_rate': 0.07990056}\n","INFO:tensorflow:Step 2200 per-step time 0.326s\n","I0929 05:23:28.754818 140223081744256 model_lib_v2.py:700] Step 2200 per-step time 0.326s\n","INFO:tensorflow:{'Loss/classification_loss': 0.16825327,\n"," 'Loss/localization_loss': 0.03683187,\n"," 'Loss/regularization_loss': 0.1499468,\n"," 'Loss/total_loss': 0.35503194,\n"," 'learning_rate': 0.07988167}\n","I0929 05:23:28.755156 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.16825327,\n"," 'Loss/localization_loss': 0.03683187,\n"," 'Loss/regularization_loss': 0.1499468,\n"," 'Loss/total_loss': 0.35503194,\n"," 'learning_rate': 0.07988167}\n","INFO:tensorflow:Step 2300 per-step time 0.327s\n","I0929 05:24:01.412144 140223081744256 model_lib_v2.py:700] Step 2300 per-step time 0.327s\n","INFO:tensorflow:{'Loss/classification_loss': 0.18694013,\n"," 'Loss/localization_loss': 0.03718464,\n"," 'Loss/regularization_loss': 0.14960971,\n"," 'Loss/total_loss': 0.37373447,\n"," 'learning_rate': 0.07986114}\n","I0929 05:24:01.412513 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.18694013,\n"," 'Loss/localization_loss': 0.03718464,\n"," 'Loss/regularization_loss': 0.14960971,\n"," 'Loss/total_loss': 0.37373447,\n"," 'learning_rate': 0.07986114}\n","INFO:tensorflow:Step 2400 per-step time 0.326s\n","I0929 05:24:34.019197 140223081744256 model_lib_v2.py:700] Step 2400 per-step time 0.326s\n","INFO:tensorflow:{'Loss/classification_loss': 0.1715386,\n"," 'Loss/localization_loss': 0.027719168,\n"," 'Loss/regularization_loss': 0.14930093,\n"," 'Loss/total_loss': 0.34855872,\n"," 'learning_rate': 0.07983897}\n","I0929 05:24:34.019585 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.1715386,\n"," 'Loss/localization_loss': 0.027719168,\n"," 'Loss/regularization_loss': 0.14930093,\n"," 'Loss/total_loss': 0.34855872,\n"," 'learning_rate': 0.07983897}\n","INFO:tensorflow:Step 2500 per-step time 0.327s\n","I0929 05:25:06.747859 140223081744256 model_lib_v2.py:700] Step 2500 per-step time 0.327s\n","INFO:tensorflow:{'Loss/classification_loss': 0.13225916,\n"," 'Loss/localization_loss': 0.028735142,\n"," 'Loss/regularization_loss': 0.1490434,\n"," 'Loss/total_loss': 0.3100377,\n"," 'learning_rate': 0.079815164}\n","I0929 05:25:06.748208 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.13225916,\n"," 'Loss/localization_loss': 0.028735142,\n"," 'Loss/regularization_loss': 0.1490434,\n"," 'Loss/total_loss': 0.3100377,\n"," 'learning_rate': 0.079815164}\n","INFO:tensorflow:Step 2600 per-step time 0.326s\n","I0929 05:25:39.304197 140223081744256 model_lib_v2.py:700] Step 2600 per-step time 0.326s\n","INFO:tensorflow:{'Loss/classification_loss': 0.16497362,\n"," 'Loss/localization_loss': 0.039246134,\n"," 'Loss/regularization_loss': 0.14870207,\n"," 'Loss/total_loss': 0.35292184,\n"," 'learning_rate': 0.07978972}\n","I0929 05:25:39.304550 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.16497362,\n"," 'Loss/localization_loss': 0.039246134,\n"," 'Loss/regularization_loss': 0.14870207,\n"," 'Loss/total_loss': 0.35292184,\n"," 'learning_rate': 0.07978972}\n","INFO:tensorflow:Step 2700 per-step time 0.326s\n","I0929 05:26:11.942725 140223081744256 model_lib_v2.py:700] Step 2700 per-step time 0.326s\n","INFO:tensorflow:{'Loss/classification_loss': 0.21044342,\n"," 'Loss/localization_loss': 0.042115867,\n"," 'Loss/regularization_loss': 0.14837107,\n"," 'Loss/total_loss': 0.40093037,\n"," 'learning_rate': 0.07976264}\n","I0929 05:26:11.943087 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.21044342,\n"," 'Loss/localization_loss': 0.042115867,\n"," 'Loss/regularization_loss': 0.14837107,\n"," 'Loss/total_loss': 0.40093037,\n"," 'learning_rate': 0.07976264}\n","INFO:tensorflow:Step 2800 per-step time 0.328s\n","I0929 05:26:44.689139 140223081744256 model_lib_v2.py:700] Step 2800 per-step time 0.328s\n","INFO:tensorflow:{'Loss/classification_loss': 0.113493115,\n"," 'Loss/localization_loss': 0.029344073,\n"," 'Loss/regularization_loss': 0.14806981,\n"," 'Loss/total_loss': 0.290907,\n"," 'learning_rate': 0.07973392}\n","I0929 05:26:44.689487 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.113493115,\n"," 'Loss/localization_loss': 0.029344073,\n"," 'Loss/regularization_loss': 0.14806981,\n"," 'Loss/total_loss': 0.290907,\n"," 'learning_rate': 0.07973392}\n","INFO:tensorflow:Step 2900 per-step time 0.326s\n","I0929 05:27:17.264582 140223081744256 model_lib_v2.py:700] Step 2900 per-step time 0.326s\n","INFO:tensorflow:{'Loss/classification_loss': 0.15495344,\n"," 'Loss/localization_loss': 0.036093198,\n"," 'Loss/regularization_loss': 0.14763995,\n"," 'Loss/total_loss': 0.3386866,\n"," 'learning_rate': 0.07970358}\n","I0929 05:27:17.265040 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.15495344,\n"," 'Loss/localization_loss': 0.036093198,\n"," 'Loss/regularization_loss': 0.14763995,\n"," 'Loss/total_loss': 0.3386866,\n"," 'learning_rate': 0.07970358}\n","INFO:tensorflow:Step 3000 per-step time 0.325s\n","I0929 05:27:49.794378 140223081744256 model_lib_v2.py:700] Step 3000 per-step time 0.325s\n","INFO:tensorflow:{'Loss/classification_loss': 0.13611935,\n"," 'Loss/localization_loss': 0.04083263,\n"," 'Loss/regularization_loss': 0.14750531,\n"," 'Loss/total_loss': 0.3244573,\n"," 'learning_rate': 0.0796716}\n","I0929 05:27:49.794809 140223081744256 model_lib_v2.py:701] {'Loss/classification_loss': 0.13611935,\n"," 'Loss/localization_loss': 0.04083263,\n"," 'Loss/regularization_loss': 0.14750531,\n"," 'Loss/total_loss': 0.3244573,\n"," 'learning_rate': 0.0796716}\n"]}]},{"cell_type":"markdown","metadata":{"id":"b-LPoWrW6fcR"},"source":["# 학습한 모델 추출(export - 모델과 가중치 함께 저장)\n","- `models/research/object_detection/exporter_main_v2.py` 사용\n","- 옵션\n","    - `exporter_main_v2.py --helpshort || exporter_main_v2.py --helpfull`\n","    - input_type : input node type\n","        - image_tensor, encoded_image_string_tensor\n","    - trained_checkpoint_dir: 학습된 checkpoint 파일이 저장된 경로(folder/directory)\n","    - pipeline_config_path: pipeline.config 파일의 경로 (파일명 포함) - 모델 구조를 알려주기 위함\n","    - output_directory: export된 모델을 저장할 경로.\n","- 추출된 디렉토리 구조\n","```bash\n","output_dir\n","├─ checkpoint/\n","├─ saved_model/\n","└─ pipeline.config\n","```\n","    - checkpoint: custom data 학습한 checkpoint 파일들을 이 디렉토리로 복사한다.\n","    - save_model: pipeline.config 설정에 맞춰 생성된 model\n","    - pipeline.config: pipeline.config 설정파일"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"S8vh7uc_CKQX","executionInfo":{"status":"ok","timestamp":1632893455196,"user_tz":-540,"elapsed":8,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"acb3fdf5-5064-4bc5-bb30-a01f00550390"},"source":["%pwd"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"4Yg0RJGh6fcR","executionInfo":{"status":"ok","timestamp":1632893739815,"user_tz":-540,"elapsed":258,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"4f512db6-8dc9-4248-a17b-258ec7318a20"},"source":["f\"!python models/research/object_detection/exporter_main_v2.py --input_type image_tensor --trained_checkpoint_dir {CHECK_POINT_PATH} --pipeline_config_path {PIPELINE_CONFIG_PATH} --output_directory {EXPORT_MODEL_PATH}\""],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!python models/research/object_detection/exporter_main_v2.py --input_type image_tensor --trained_checkpoint_dir /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/checkpoint --pipeline_config_path /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/pipeline.config --output_directory /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/export_model'"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kIWyQnhp6fcR","executionInfo":{"status":"ok","timestamp":1632893825596,"user_tz":-540,"elapsed":78993,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"c9c90c50-f0ad-4d04-ce41-dbed3ba2c47b"},"source":["!python models/research/object_detection/exporter_main_v2.py --input_type image_tensor --trained_checkpoint_dir /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/checkpoint --pipeline_config_path /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/pipeline.config --output_directory /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/export_model"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-09-29 05:35:46.883715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:35:46.896634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:35:46.897462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:35:46.911813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:35:46.912801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:35:46.913642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:35:47.401047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:35:47.401823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:35:47.402597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-09-29 05:35:47.403311: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2021-09-29 05:35:47.403416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10819 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.map_fn(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n","W0929 05:35:47.590194 140338957653888 deprecation.py:616] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.map_fn(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7fa2c8a40690>, because it is not built.\n","W0929 05:36:14.897712 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7fa2c8a40690>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7fa2c01e6050>, because it is not built.\n","W0929 05:36:15.167557 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7fa2c01e6050>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa2c0036f50>, because it is not built.\n","W0929 05:36:15.167848 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa2c0036f50>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24ff55d10>, because it is not built.\n","W0929 05:36:15.168025 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24ff55d10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7fa24ff55a10>, because it is not built.\n","W0929 05:36:15.168188 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7fa24ff55a10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24ff55950>, because it is not built.\n","W0929 05:36:15.168344 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24ff55950>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24f97c290>, because it is not built.\n","W0929 05:36:15.168514 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24f97c290>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7fa24fb345d0>, because it is not built.\n","W0929 05:36:15.168668 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7fa24fb345d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa250008e90>, because it is not built.\n","W0929 05:36:15.168833 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa250008e90>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24f97c490>, because it is not built.\n","W0929 05:36:15.168998 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24f97c490>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7fa24ffaa050>, because it is not built.\n","W0929 05:36:15.169148 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7fa24ffaa050>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24f98f750>, because it is not built.\n","W0929 05:36:15.169304 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24f98f750>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24f98fb10>, because it is not built.\n","W0929 05:36:15.169470 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24f98fb10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa2c01e6e10>, because it is not built.\n","W0929 05:36:15.169618 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa2c01e6e10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fe51c90>, because it is not built.\n","W0929 05:36:15.169767 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fe51c90>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24f98a210>, because it is not built.\n","W0929 05:36:15.169927 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24f98a210>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fd37410>, because it is not built.\n","W0929 05:36:15.170109 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fd37410>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fd37510>, because it is not built.\n","W0929 05:36:15.170333 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fd37510>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fd37890>, because it is not built.\n","W0929 05:36:15.170493 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fd37890>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24f9f7390>, because it is not built.\n","W0929 05:36:15.170641 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24f9f7390>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fca3f90>, because it is not built.\n","W0929 05:36:15.170863 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fca3f90>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa2c01e6e50>, because it is not built.\n","W0929 05:36:15.171015 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa2c01e6e50>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fbde1d0>, because it is not built.\n","W0929 05:36:15.171185 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fbde1d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fbdef50>, because it is not built.\n","W0929 05:36:15.171391 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fbdef50>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fbde910>, because it is not built.\n","W0929 05:36:15.171540 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fbde910>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fbded90>, because it is not built.\n","W0929 05:36:15.171713 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fbded90>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fbde8d0>, because it is not built.\n","W0929 05:36:15.171890 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fbde8d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fbde850>, because it is not built.\n","W0929 05:36:15.172041 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fbde850>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fbef310>, because it is not built.\n","W0929 05:36:15.172196 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fbef310>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fb5b650>, because it is not built.\n","W0929 05:36:15.172367 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fb5b650>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24faefe90>, because it is not built.\n","W0929 05:36:15.172626 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24faefe90>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24faeff10>, because it is not built.\n","W0929 05:36:15.172834 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24faeff10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fa18790>, because it is not built.\n","W0929 05:36:15.173047 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fa18790>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fa188d0>, because it is not built.\n","W0929 05:36:15.173279 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fa188d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fa18710>, because it is not built.\n","W0929 05:36:15.173515 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fa18710>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fef92d0>, because it is not built.\n","W0929 05:36:15.173702 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fef92d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fb5cc50>, because it is not built.\n","W0929 05:36:15.174171 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fb5cc50>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa2c01e6e90>, because it is not built.\n","W0929 05:36:15.174362 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa2c01e6e90>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fa1b3d0>, because it is not built.\n","W0929 05:36:15.174608 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fa1b3d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fa1bd50>, because it is not built.\n","W0929 05:36:15.174863 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fa1bd50>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fa1be50>, because it is not built.\n","W0929 05:36:15.175109 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fa1be50>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa25007f490>, because it is not built.\n","W0929 05:36:15.175350 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa25007f490>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fb08dd0>, because it is not built.\n","W0929 05:36:15.175579 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fb08dd0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fa2add0>, because it is not built.\n","W0929 05:36:15.175789 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7fa24fa2add0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fa2a790>, because it is not built.\n","W0929 05:36:15.176005 140338957653888 save_impl.py:72] Skipping full serialization of Keras layer <keras.layers.core.Lambda object at 0x7fa24fa2a790>, because it is not built.\n","2021-09-29 05:36:27.725380: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n","W0929 05:36:53.971304 140338957653888 save.py:254] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxPredictor_layer_call_fn while saving (showing 5 of 260). These functions will not be directly callable after loading.\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/export_model/saved_model/assets\n","I0929 05:36:59.989504 140338957653888 builder_impl.py:781] Assets written to: /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/export_model/saved_model/assets\n","INFO:tensorflow:Writing pipeline config file to /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/export_model/pipeline.config\n","I0929 05:37:00.886848 140338957653888 config_util.py:254] Writing pipeline config file to /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/export_model/pipeline.config\n"]}]},{"cell_type":"markdown","metadata":{"id":"IqGfDAst6fcR"},"source":["# Inference(추론)"]},{"cell_type":"markdown","metadata":{"id":"9YpFpeYO6fcR"},"source":["### 사용 함수,메소드\n","-  ### tf.convert_to_tensor(array_like, dtype)\n","    - array_like 를 Tensorflow Tensor 객체로 변환\n","    - `tf.convert_to_tensor([[1,2],[3,4]])`\n","- ### detection_model.preprocess(image 4차원 <del>ndarray</del> Tensor)\n","    - 전달받은 이미지를 model의 input shape(320,320)에 맞게 resizing 한다.\n","    - 반환값: (resize된 image Tensor, 이미지의 shape) 을 tuple로 반환\n","- ### detection_model.predict(image tensor, image_shape tensor)\n","    - 추론/detection 메소드\n","    - 이미지와 image shape을 받아서 detection한 결과를 딕셔너리로 반환한다.\n","    - **반환 dictionary key**\n","        - **preprocessed_inputs**:  입력 이미지 Tensor. preprocess()로 처리된 이미지. \n","        - **feature_maps**: List. feature map 들을 반환\n","        - **anchors**: 2D Tensor. normalize 된 anchor box들의 좌표를 반환. 2-D float tensor: \\[num_anchors, 4\\]\n","        - **final_anchors**: 3D Tensor. batch 당 anchors. (anchors에 batch가 포함된 것). \\[batch_size, num_anchors, 4\\]\n","        - **box_encodings**: 3D float tensor. predict한 box들의 normalize된 좌표. \\[batch_size, num_anchors,box_code_dimension\\]\n","        - **class_predictions_with_background**: 3D Tensor. 클래스 확률을 반환.(logit). \\[batch_size, num_anchors, num_classes+1]\\\n","            - background 확률을 포함해서 num_classes+1개가 된다. (index 0: background)\n","            \n","- ### detection_model.postprocess(prediction_dict, shape)\n","    - predict()가 예측한 결과에서 **Non-Maxinum Suppression**을 실행해서 최종 Detection 결과를 반환한다.\n","        - predict()는 anchor별로 예측결과를 모아서 주고 post-process는 최종 결과를 추출해서 반환.\n","    - **반환 dictionary key**\n","        - **num_detections**: Detect한 개수 (bounding box 개수)\n","        - **detection_boxes**: [batch, max_detections, 4]. 후처리한 detection box\n","        - **detection_scores**: [batch, max_detections]. post-processed detection box들의 detection score들 (detection score는 box안에 물체가 있을 확률값 - confidence score).\n","        - **detection_classes**: [batch, max_detections] tensor with classes for post-processed detection classes.\n","        - **raw_detection_boxes**:[batch, total_detections, 4] Non-Max Suppression 하기 전의 감지된 box들\n","        - **raw_detection_scores**: [batch, total_detections, num_classes_with_background]. raw detection box들의 class별 점수\n","        - **detection_multiclass_scores**: [batch, max_detections, num_classes_with_background] post-processed이후 남은 bounding box 들의 class별 점수. LabelMap의 class에 background가 추가되어 계산된다.\n","        - **detection_anchor_indices**: [batch, max_detections] post-processed 이후 나은 anchor box의 index들."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"vGMS_WC46fcS","executionInfo":{"status":"ok","timestamp":1632895788878,"user_tz":-540,"elapsed":9,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}},"outputId":"5768072b-4370-4c83-be2b-f8a8e9f0252c"},"source":["# checkpoints.zip 압축풀기\n","f\"!unzip -q {os.path.join(MODEL_PATH, 'checkpoint_backup', 'checkpoints.zip')} -d {os.path.join(MODEL_PATH, 'checkpoint_backup')}\""],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!unzip -q /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/checkpoint_backup/checkpoints.zip -d /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/checkpoint_backup'"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"OoQ6c2Ep6fcS","executionInfo":{"status":"ok","timestamp":1632894414355,"user_tz":-540,"elapsed":5093,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["!unzip -q /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/checkpoint_backup/checkpoints.zip -d /content/drive/MyDrive/ColabNotebooks/object_detection_src/sign_language_letters/workspace/model/checkpoint_backup"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"rem-JSZj6fcS","executionInfo":{"status":"ok","timestamp":1632896039990,"user_tz":-540,"elapsed":247,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["# 추론할 이미지를 test set으로부터 복사\n","!cp /content/images/test/A22_jpg.rf.f02ad8558ce1c88213b4f83c0bc66bc8.jpg ./a.jpg"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJ42OD5s6fcS","executionInfo":{"status":"ok","timestamp":1632896129872,"user_tz":-540,"elapsed":705,"user":{"displayName":"이홍주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15116000017939649577"}}},"source":["!cp /content/images/test/C22_jpg.rf.e54cbbfdd4ea0670eb4e1c507de4a8a2.jpg ./c.jpg\n","!cp /content/images/test/F17_jpg.rf.6097db79e0385af55b85ad5fa03cdc55.jpg ./f.jpg\n","!cp /content/images/test/G3_jpg.rf.e723dcdc277f3432e4eb7003b6e5a587.jpg ./g.jpg"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kr0wKEkrLzKy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTpsr2BFNl6J"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZntzZOW16fcT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"33UiS7wC6fcT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o3i6cXbs6fcT"},"source":["# 새로운 이미지 Detection"]},{"cell_type":"code","metadata":{"id":"JQfkZA4J6fcT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ww9M9mih6fcT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iZ1Ncfox6fcT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BggKRZjt6fcT"},"source":[""],"execution_count":null,"outputs":[]}]}