
실시간 크롤링 데이터 수집 -> 정제 -> csv 파일로 변환 -> ES에 넣어야 할 경우 자동화로 처리
파일을 es에 직접 저장하는 기능 활용 : beats가 file을 read해서 logstash에 넣음
logstash의 설정 정보에 의해 데이터가 일정부분 filtering됨
이를 ES에 넣는 작업을 자동화
저장된 데이터로 kibana에서 시각화, 데이터가 갱신되면 실시간 차트도 갱신됨
html의 응답 차트로 실시간 적용 가능

Pipeline: 한 데이터 처리 단계의 출력이 다음 단계의 입력으로 이어지는 구조

ES Pipeline 구축 실습
    - 데이터 수집: Beats --> 필터링: logstash --> 데이터 저장: ES --> 시각화: kibana
    
환경 구축
    logstash > config > covid-19.conf 설정파일
    실행 명령어 - 순서 중요
        logstash -f ...~

    filebeat.yml: 인간 친화적인 설정 파일. 들여쓰기가 문법
    파일 저장 위치, es에 보내는 부분 명시

covid-19.csv 파일의 데이터 사용
    1단계: logstash에도 없고 es에도 없던 데이터가 인식
        filebeat이 한 번에 저장 시도해서 성공

    2단계: 이미 존재하는 file의 마지막 부분에 새로운 데이터 추가 저장
        filebeat이 추가된 데이터에 한해서만 새로 저장
        기존에 이미 있는 것까지는 인식하고 있음

    3단계: 2단계에서 추가한 데이터 csv 파일 상에서 다시 삭제함
        filebeat은 새로 logstash에 반영하지 않음
        이미 18개 삭제 이전의 데이터는 logstash에 저장되어 있기 때문
    
    결론: 추가하는 데이터 새로 저장은 가능
    이미 존재하는 데이터 값은 변경하지 않는 한 그대로 유지

    참고)
        기존 데이터 수정할 경우 가장 안정적인 방법은 백업된 모든 데이터를 다시 저장하는 것을 더 선호
        수정된다는 것을 100% 보장할 수 없기 때문
        이미 적재한 데이터를 수정 시에는 ES 자체적으로 수정 권장
        filebeats 통해서 실시간 가변적인 새로운 데이터 적재(저장) 시도는 ok
        이미 저장시킨 데이터 수정 시도의 action은 비추