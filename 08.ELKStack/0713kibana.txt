GET test-index/_mapping
GET test-index/_search
GET test-index/_search
{
  "query": {
    "match_all": {}
  }
}

PUT test-index/_doc/

GET bank/_search
{
  "size": 0, 
  "query": {
    "match": {
      "bank": "국민은행"
    }
  },
  "aggs": {
    "kb_cus": {
      "sum": {
        "field": "customers"
      }
    }
  }
}

GET bank/_search
{
  "query": {
    "match": {
      "bank": "국민은행"
    }
  },
  "size": 0, 
  "aggs": {
    "loc_count": {
      "terms": {
        "field": "location.keyword"
      }
    }
  }
}

GET bank/_search
{
  "size": 0, 
  "aggs": {
    "b_2": {
      "cardinality": {
        "field": "branch.keyword"
      }
    }
  }
}

# 한글 형태소 분석
# https://www.elastic.co/guide/en/elasticsearch/plugins/7.11/analysis-nori-tokenizer.html

# standard는 기본적으로 여백을 구분자로 해서 토큰 분해
# utf-8 기반의 문자들은 인식
GET _analyze
{
  "tokenizer": "standard",
  "text": [
    "동해물과 백두산이"
  ]
}

# nori 플러그인 설치 후에 실행되는 문장들
# tokenizer 파라미터에 따라 분석이 달라짐
# decompound_mode 와 user_dictionary에 따라 다름
# user_dictionary에 사용되는 파일명은 자유, 위치는 elasticsearch > config에 있어야 함
# 권장 파일명은 userdict_ko.txt

PUT nori_sample
{
  "settings": {
    "index": {
      "analysis": {
        "tokenizer": {
          "nori_user_dict": {
            "type": "nori_tokenizer",
            "decompound_mode": "mixed",
            "discard_punctuation": "false",
            "user_dictionary": "userdict_ko.txt"
          }
        },
        "analyzer": {
          "my_analyzer": {
            "type": "custom",
            "tokenizer": "nori_user_dict"
          }
        }
      }
    }
  }
}

GET nori_sample/_analyze
{
  "analyzer": "my_analyzer",
  "text": "세종시"  
}

GET _analyze
{
  "tokenizer": "nori_tokenizer",
  "text": "뒷동산에 감나무 심기"
}

GET _analyze
{
  "tokenizer": "standard",
  "text": "뒷동산에 감나무 심기"
}

GET _analyze
{
  "tokenizer": "nori_tokenizer",
  "text": "뒷동산에 @ 감나무 심기"
}

GET _analyze
{
  "tokenizer": "standard",
  "text": "뒷동산에 @ 감나무 심기"
}

GET _analyze
{
  "tokenizer": "whitespace",
  "text": "뒷동산에 @ 감나무 심기"
}

DELETE my_nori

# index에 사전 추가 작업
PUT my_nori
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "my_n_t": {
          "type": "nori_tokenizer",
          "user_dictionary_rules":[
            "뒷동산"  
          ]
        }
      }
    }
  }
}

# "감나무" ==> 뒷/동산/에/감나무/심기
# "뒷동산" ==> 뒷동산/에/감/나무/심기
GET my_nori/_analyze
{
  "tokenizer": "my_n_t",
  "text": "뒷동산에 @ 감나무 심기"
}

# nori 토크나이저 편집
DELETE my_nori

PUT my_nori
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "nori_none": {
          "type": "nori_tokenizer",
          "decompound_mode": "none"
        },
        "nori_discard": {
          "type": "nori_tokenizer",
          "decompound_mode": "discard"
        },
        "nori_mixed": {
          "type": "nori_tokenizer",
          "decompound_mode": "mixed"
        }
      }
    }
  }
}

GET my_nori/_analyze
{
  "tokenizer": "nori_none",
  "text": "감나무"
}

GET my_nori/_analyze
{
  "tokenizer": "nori_discard",
  "text": "감나무"
}

GET my_nori/_analyze
{
  "tokenizer": "nori_mixed",
  "text": "감나무"
}

GET my_nori/_analyze
{
  "tokenizer": "nori_none",
  "text": "뒷동산에 감나무 심기"
}

GET my_nori/_analyze
{
  "tokenizer": "nori_discard",
  "text": "1994년 뒷동산에 올라가서 감나무를 심고 다시 내려왔다."
}

GET my_nori/_analyze
{
  "tokenizer": "nori_mixed",
  "text": "1994년 뒷동산에 올라가서 감나무를 심고 다시 내려왔다."
}


# 사용자 정의 사전 추가
# 위치 : ES/config/*_ko.txt
# tokenizer와 analyzer 동시 설정
# "user_dictionary": "userdict_ko.txt"
# ES는 config 하위의 파일 자동 인식
DELETE my_nori
PUT my_nori
{
  "settings": {
    "index": {
      "analysis": {
        "tokenizer":{
          "nori_user_dict": {
            "type": "nori_tokenizer",
            "decompound_mode": "mixed",
            "user_dictionary": "userdict_ko.txt"
          }
        },
        "analyzer": {
          "nori_my_token":{
            "type": "custom",
            "tokenizer": "nori_user_dict"
          }
        }
      }
    }
  }
}
# 엘지전자 입력시에 tokenizer 에 mixed 라고 해도 엘지전자 는 휘발이 되고 "엘지", "전자"로만 검색
# 해결책 - 사용자 정의 사전에 엘지전자 내용 추가
# 추가하는 방식 : 엘지전자 엘지 전자  형식
# 단, 혹여 복합명사 인식이 잘 안 될 경우에 한번 더 복합명사 적용 권장
# 사용자 정의 사전에 추가한 문구
# 엘지전자,
# 엘지전자 엘지 전자
GET my_nori/_analyze
{
  "analyzer": "nori_my_token",
  "text": "엘지전자"
}
